{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T10:29:39.640455Z",
     "start_time": "2018-12-08T10:29:35.460018Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score,log_loss\n",
    "import time\n",
    "import os\n",
    "# import tf2onnx\n",
    "import sys\n",
    "import json\n",
    "from collections import deque\n",
    "from functools import reduce\n",
    "import functools\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm as batch_norm\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T10:29:39.651228Z",
     "start_time": "2018-12-08T10:29:39.642095Z"
    },
    "code_folding": [
     0
    ],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class config_midas(object):\n",
    "    # input\n",
    "    _basePath = \"/home/zhoutong/data/apus_ad/midas/tfrecord_2018-11-01_to_2018-11-23_and_2018-11-24_to_2018-11-30_filterRepeatView_intersectLR\"\n",
    "    train_tfrecord_file = _basePath+\"/train.tfrecord.gz\"\n",
    "    valid_tfrecord_file = _basePath+\"/valid.tfrecord.gz\"\n",
    "    info_file = _basePath+\"/info.json\"\n",
    "    # output\n",
    "    tagTime= time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime(time.time()))\n",
    "    base_save_dir = \"/home/zhoutong/tf_modelInfo/type={type}/dt={dt}\".format(type=\"midas\",dt=tagTime)\n",
    "    # load-json\n",
    "    with open(info_file,\"r+\") as f:\n",
    "        info = \"\".join(f.readlines())\n",
    "        result = json.loads(info)\n",
    "\n",
    "    fieldInfo = result['allField']\n",
    "    statisticInfo = result['statistic']\n",
    "    tmp_map_num_f = result['numericFieldMap']#{'ad_info__budget_unit':1291744}\n",
    "    max_numeric = result['numericMax']#{\"ad_info__budget_unit\": 2.0}\n",
    "\n",
    "    # 连续特征的索引号要单独给出来，方便后续构造idx_sparse_tensor\n",
    "    # 关于这里的filter: spark处理空数组生成JSON的问题, Seq().mkString 仍会产生一个空串，在这里要去除掉\n",
    "    data_param_dicts = {\n",
    "        \"global_numeric_fields\":list(filter(lambda x: x!=\"\", fieldInfo['numeric_fields'].split(\",\"))),\n",
    "        \"global_multi_hot_fields\":list(filter(lambda x: x!=\"\", fieldInfo['multi_hot_fields'].split(\",\"))),\n",
    "        \"global_all_fields\" : list(filter(lambda x: x!=\"\", fieldInfo['all_fields'].split(\",\"))),\n",
    "        \"tmp_map_num_f\": result['numericFieldMap'],\n",
    "        \"max_numeric\" : result['numericMax']\n",
    "    }\n",
    "    # 如果没有使用numeric 或者 multi_hot特征,会自动构造一个不起作用的numeric(multi_hot)特征,所以size要置为1\n",
    "    data_param_dicts[\"numeric_field_size\"] = len(data_param_dicts['global_numeric_fields']) if len(data_param_dicts['global_numeric_fields']) >0 else 1\n",
    "    data_param_dicts[\"multi_hot_field_size\"] = len(data_param_dicts['global_multi_hot_fields']) if len(data_param_dicts['global_multi_hot_fields']) >0 else 1\n",
    "\n",
    "\n",
    "    # 调参修正如下参数\n",
    "    deepfm_param_dicts = {\n",
    "        \"dropout_fm\" : [1.0, 1.0],\n",
    "        \"dropout_deep\" : [0.8, 0.9, 0.9, 0.9, 0.9],\n",
    "        \"feature_size\": statisticInfo['feature_size']+1,\n",
    "        \"batch_size\":1024*3,\n",
    "        \"embedding_size\": 2,\n",
    "        \"epoch\":8,\n",
    "        \"deep_layers_activation\" : tf.nn.relu,\n",
    "        \"batch_norm_decay\": 0.9,\n",
    "        \"deep_layers\":[16,8],\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"l2_reg\":0.001\n",
    "    }\n",
    "\n",
    "    random_seed=2017\n",
    "    gpu_num=1\n",
    "    is_debug=False\n",
    "\n",
    "    # @staticmethod\n",
    "    # def get_now():\n",
    "    #     return time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime(time.time()))\n",
    "    # @staticmethod\n",
    "    # def get_dict(instance:object):\n",
    "    #     keys = [attr for attr in dir(instance) if not callable(getattr(instance, attr)) and not attr.startswith(\"__\")]\n",
    "    #     return {key:getattr(instance,key) for key in keys}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T10:29:39.659266Z",
     "start_time": "2018-12-08T10:29:39.652591Z"
    },
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "CONFIG = config_midas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T10:29:39.692188Z",
     "start_time": "2018-12-08T10:29:39.662284Z"
    },
    "code_folding": [
     3,
     6,
     12
    ],
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic --\n",
      "     train_pos = 2555919\n",
      "     valid_neg = 689001\n",
      "     train_neg = 3641916\n",
      "     multi_hot_f_size = 2\n",
      "     field_size = 69\n",
      "     valid_pos = 466749\n",
      "     feature_size = 1999613\n",
      "     numeric_f_size = 46\n",
      "numericFieldMap --\n",
      "     stat_advertising_id_s__cvr_7d = 1119260\n",
      "     stat_ad_app_s__cvr_3d = 629818\n",
      "     stat_advertising_id_s__view_15d = 1829594\n",
      "     stat_advertising_id_s__install_15d = 1039436\n",
      "     stat_advertising_id_s__ctr_15d = 189778\n",
      "     stat_advertising_id_s__cvr_3d = 1859604\n",
      "     stat_ad_app_s__cvr_7d = 309774\n",
      "     stat_advertising_id_s__install_3d = 1559863\n",
      "     stat_ad_app_s__install_3d = 849375\n",
      "     stat_advertising_id_s__click_3d = 1129297\n",
      "     stat_ad_creative_id_s__click_3d = 1179540\n",
      "     stat_ad_creative_id_s__install_15d = 1669363\n",
      "     stat_ad_app_s__view_15d = 1049262\n",
      "     stat_advertising_id_s__install_7d = 1499752\n",
      "     stat_ad_app_s__view_7d = 1399797\n",
      "     stat_ad_app_s__install_7d = 519803\n",
      "     stat_advertising_id_s__click_7d = 749649\n",
      "     stat_ad_creative_id_s__click_7d = 779610\n",
      "     stat_ad_app_s__view_3d = 199719\n",
      "     stat_ad_creative_id_s__ctr_3d = 29966\n",
      "     stat_ad_creative_id_s__ctr_7d = 789563\n",
      "     stat_ad_creative_id_s__install_3d = 429571\n",
      "     stat_ad_app_s__ctr_15d = 329794\n",
      "     stat_ad_app_s__install_15d = 639805\n",
      "     stat_ad_creative_id_s__ctr_15d = 269861\n",
      "     stat_ad_creative_id_s__cvr_15d = 1709685\n",
      "     stat_advertising_id_s__view_3d = 1929515\n",
      "     stat_advertising_id_s__ctr_3d = 1379637\n",
      "     stat_ad_app_s__click_15d = 289698\n",
      "     stat_advertising_id_s__click_15d = 1039435\n",
      "     stat_advertising_id_s__view_7d = 1989572\n",
      "     stat_advertising_id_s__ctr_7d = 1199542\n",
      "     stat_ad_creative_id_s__install_7d = 779611\n",
      "     stat_ad_app_s__ctr_3d = 1419788\n",
      "     stat_ad_creative_id_s__click_15d = 899195\n",
      "     stat_ad_creative_id_s__cvr_7d = 1229511\n",
      "     stat_ad_app_s__click_3d = 329793\n",
      "     stat_ad_app_s__cvr_15d = 1829595\n",
      "     stat_ad_creative_id_s__view_3d = 1749565\n",
      "     stat_advertising_id_s__cvr_15d = 369887\n",
      "     stat_ad_creative_id_s__cvr_3d = 690078\n",
      "     stat_ad_app_s__click_7d = 699981\n",
      "     stat_ad_creative_id_s__view_7d = 879217\n",
      "     stat_ad_creative_id_s__view_15d = 489797\n",
      "     stat_ad_app_s__ctr_7d = 1589942\n",
      "     ad_info__budget_unit = 1779711\n",
      "allField --\n",
      "     all_fields = label,stat_ad_creative_id_s__install_15d,stat_advertising_id_s__cvr_3d,stat_ad_app_s__install_3d,stat_ad_creative_id_s__ctr_3d,stat_advertising_id_s__ctr_15d,ad_info__advertiser_type,stat_ad_creative_id_s__cvr_7d,ad_info__company_full_name,stat_ad_app_s__ctr_3d,stat_ad_creative_id_s__ctr_7d,stat_advertising_id_s__click_15d,stat_ad_app_s__cvr_15d,user_behavior__country_s,ad_info__advertiser_id,stat_advertising_id_s__cvr_15d,user_behavior__client_ip_s,user_behavior__net_s,stat_ad_app_s__install_7d,stat_ad_creative_id_s__view_3d,stat_advertising_id_s__view_7d,user_behavior__channel_id_s,stat_ad_app_s__view_15d,stat_advertising_id_s__install_7d,ad_info__app_package_name,ad_info__ad_id,stat_ad_creative_id_s__install_3d,stat_advertising_id_s__install_15d,stat_advertising_id_s__ctr_7d,stat_ad_creative_id_s__cvr_15d,stat_ad_creative_id_s__view_7d,stat_advertising_id_s__view_15d,stat_ad_app_s__cvr_7d,stat_ad_app_s__click_7d,stat_advertising_id_s__cvr_7d,stat_advertising_id_s__install_3d,stat_ad_app_s__view_3d,stat_ad_creative_id_s__ctr_15d,stat_ad_app_s__install_15d,ad_info__ad_creative_id_s,ad_info__activity_id,stat_ad_app_s__ctr_7d,user_behavior__app_version_s,stat_advertising_id_s__ctr_3d,stat_ad_creative_id_s__click_15d,stat_ad_creative_id_s__view_15d,ad_info__ad_type,stat_advertising_id_s__click_3d,stat_ad_app_s__cvr_3d,user_behavior__package_name_s,user_profile_tag__tag_list,stat_ad_app_s__click_3d,ad_info__budget_unit,ad_info__budget_type,stat_advertising_id_s__view_3d,stat_ad_creative_id_s__click_7d,stat_ad_creative_id_s__click_3d,ad_info__gp_category,user_behavior__hour_s,stat_ad_creative_id_s__cvr_3d,stat_ad_app_s__ctr_15d,user_behavior__ad_position_id_s,stat_advertising_id_s__click_7d,stat_ad_creative_id_s__install_7d,stat_ad_app_s__view_7d,user_behavior__api_version_s,user_behavior__language_s,stat_ad_app_s__click_15d\n",
      "     numeric_fields = ad_info__budget_unit,stat_advertising_id_s__view_3d,stat_advertising_id_s__click_3d,stat_advertising_id_s__install_3d,stat_advertising_id_s__ctr_3d,stat_advertising_id_s__cvr_3d,stat_advertising_id_s__view_7d,stat_advertising_id_s__click_7d,stat_advertising_id_s__install_7d,stat_advertising_id_s__ctr_7d,stat_advertising_id_s__cvr_7d,stat_advertising_id_s__view_15d,stat_advertising_id_s__click_15d,stat_advertising_id_s__install_15d,stat_advertising_id_s__ctr_15d,stat_advertising_id_s__cvr_15d,stat_ad_creative_id_s__view_3d,stat_ad_creative_id_s__click_3d,stat_ad_creative_id_s__install_3d,stat_ad_creative_id_s__ctr_3d,stat_ad_creative_id_s__cvr_3d,stat_ad_creative_id_s__view_7d,stat_ad_creative_id_s__click_7d,stat_ad_creative_id_s__install_7d,stat_ad_creative_id_s__ctr_7d,stat_ad_creative_id_s__cvr_7d,stat_ad_creative_id_s__view_15d,stat_ad_creative_id_s__click_15d,stat_ad_creative_id_s__install_15d,stat_ad_creative_id_s__ctr_15d,stat_ad_creative_id_s__cvr_15d,stat_ad_app_s__view_3d,stat_ad_app_s__click_3d,stat_ad_app_s__install_3d,stat_ad_app_s__ctr_3d,stat_ad_app_s__cvr_3d,stat_ad_app_s__view_7d,stat_ad_app_s__click_7d,stat_ad_app_s__install_7d,stat_ad_app_s__ctr_7d,stat_ad_app_s__cvr_7d,stat_ad_app_s__view_15d,stat_ad_app_s__click_15d,stat_ad_app_s__install_15d,stat_ad_app_s__ctr_15d,stat_ad_app_s__cvr_15d\n",
      "     multi_hot_fields = user_profile_tag__tag_list\n",
      "featureCnt --\n",
      "     stat_advertising_id_s__cvr_7d = 2\n",
      "     stat_ad_app_s__cvr_3d = 2\n",
      "     stat_advertising_id_s__view_15d = 2\n",
      "     stat_advertising_id_s__cvr_3d = 2\n",
      "     stat_ad_app_s__cvr_7d = 2\n",
      "     stat_ad_app_s__install_3d = 2\n",
      "     stat_advertising_id_s__install_3d = 2\n",
      "     stat_ad_creative_id_s__install_15d = 2\n",
      "     stat_ad_creative_id_s__click_3d = 2\n",
      "     stat_ad_app_s__view_15d = 2\n",
      "     stat_ad_app_s__install_7d = 2\n",
      "     stat_advertising_id_s__install_7d = 2\n",
      "     stat_ad_app_s__view_7d = 2\n",
      "     user_behavior__language_s = 116\n",
      "     stat_ad_creative_id_s__click_7d = 2\n",
      "     stat_ad_app_s__view_3d = 2\n",
      "     user_behavior__ad_position_id_s = 1295\n",
      "     user_behavior__client_ip_s = 853815\n",
      "     stat_ad_creative_id_s__install_3d = 2\n",
      "     stat_ad_app_s__install_15d = 2\n",
      "     stat_ad_creative_id_s__ctr_15d = 2\n",
      "     ad_info__app_package_name = 9\n",
      "     ad_info__ad_creative_id_s = 42\n",
      "     stat_advertising_id_s__ctr_3d = 2\n",
      "     stat_advertising_id_s__view_3d = 2\n",
      "     ad_info__budget_type = 4\n",
      "     stat_advertising_id_s__view_7d = 2\n",
      "     stat_advertising_id_s__ctr_7d = 2\n",
      "     ad_info__gp_category = 3\n",
      "     stat_ad_creative_id_s__install_7d = 2\n",
      "     user_behavior__package_name_s = 107\n",
      "     user_profile_tag__tag_list = 50\n",
      "     user_behavior__channel_id_s = 1315\n",
      "     stat_ad_app_s__cvr_15d = 2\n",
      "     stat_ad_app_s__click_3d = 2\n",
      "     stat_ad_creative_id_s__view_3d = 2\n",
      "     ad_info__activity_id = 18\n",
      "     ad_info__ad_id = 42\n",
      "     stat_ad_app_s__click_7d = 2\n",
      "     stat_ad_creative_id_s__view_7d = 2\n",
      "     user_behavior__app_version_s = 364\n",
      "     user_behavior__hour_s = 25\n",
      "     user_behavior__api_version_s = 4\n",
      "     stat_advertising_id_s__install_15d = 2\n",
      "     stat_advertising_id_s__ctr_15d = 2\n",
      "     stat_advertising_id_s__click_3d = 2\n",
      "     stat_advertising_id_s__click_7d = 2\n",
      "     user_behavior__net_s = 8\n",
      "     stat_ad_creative_id_s__ctr_3d = 2\n",
      "     stat_ad_creative_id_s__ctr_7d = 2\n",
      "     ad_info__advertiser_id = 9\n",
      "     ad_info__company_full_name = 5\n",
      "     stat_ad_app_s__ctr_15d = 2\n",
      "     stat_ad_creative_id_s__cvr_15d = 2\n",
      "     stat_ad_app_s__click_15d = 2\n",
      "     user_behavior__country_s = 238\n",
      "     stat_advertising_id_s__click_15d = 2\n",
      "     stat_ad_app_s__ctr_3d = 2\n",
      "     stat_ad_creative_id_s__click_15d = 2\n",
      "     ad_info__advertiser_type = 2\n",
      "     stat_ad_creative_id_s__cvr_7d = 2\n",
      "     stat_advertising_id_s__cvr_15d = 2\n",
      "     stat_ad_creative_id_s__cvr_3d = 2\n",
      "     ad_info__ad_type = 2\n",
      "     stat_ad_creative_id_s__view_15d = 2\n",
      "     stat_ad_app_s__ctr_7d = 2\n",
      "     ad_info__budget_unit = 2\n",
      "numericMax --\n",
      "     stat_advertising_id_s__cvr_7d = 3\n",
      "     stat_ad_app_s__cvr_3d = 0\n",
      "     stat_advertising_id_s__view_15d = 49266\n",
      "     stat_advertising_id_s__install_15d = 7\n",
      "     stat_advertising_id_s__ctr_15d = 1\n",
      "     stat_advertising_id_s__cvr_3d = 2\n",
      "     stat_ad_app_s__cvr_7d = 0\n",
      "     stat_advertising_id_s__install_3d = 3\n",
      "     stat_ad_app_s__install_3d = 0\n",
      "     stat_advertising_id_s__click_3d = 603\n",
      "     stat_ad_creative_id_s__click_3d = 1039994\n",
      "     stat_ad_creative_id_s__install_15d = 0\n",
      "     stat_ad_app_s__view_15d = 53032547.0\n",
      "     stat_advertising_id_s__install_7d = 3\n",
      "     stat_ad_app_s__view_7d = 43658681.0\n",
      "     stat_ad_app_s__install_7d = 0\n",
      "     stat_advertising_id_s__click_7d = 970\n",
      "     stat_ad_creative_id_s__click_7d = 1874363\n",
      "     stat_ad_app_s__view_3d = 22980244.0\n",
      "     stat_ad_creative_id_s__ctr_3d = 0.225\n",
      "     stat_ad_creative_id_s__ctr_7d = 0.1511111111111111\n",
      "     stat_ad_creative_id_s__install_3d = 0\n",
      "     stat_ad_app_s__ctr_15d = 0.10975034472748385\n",
      "     stat_ad_app_s__install_15d = 0\n",
      "     stat_ad_creative_id_s__ctr_15d = 0.13494809688581316\n",
      "     stat_ad_creative_id_s__cvr_15d = 0\n",
      "     stat_advertising_id_s__view_3d = 20121\n",
      "     stat_advertising_id_s__ctr_3d = 1\n",
      "     stat_ad_app_s__click_15d = 2508031\n",
      "     stat_advertising_id_s__click_15d = 2519\n",
      "     stat_advertising_id_s__view_7d = 24026\n",
      "     stat_advertising_id_s__ctr_7d = 1\n",
      "     stat_ad_creative_id_s__install_7d = 0\n",
      "     stat_ad_app_s__ctr_3d = 0.2\n",
      "     stat_ad_creative_id_s__click_15d = 4060052\n",
      "     stat_ad_creative_id_s__cvr_7d = 0\n",
      "     stat_ad_app_s__click_3d = 1039994\n",
      "     stat_ad_app_s__cvr_15d = 0\n",
      "     stat_ad_creative_id_s__view_3d = 22980244.0\n",
      "     stat_advertising_id_s__cvr_15d = 2.3333333333333335\n",
      "     stat_ad_creative_id_s__cvr_3d = 0\n",
      "     stat_ad_app_s__click_7d = 1874363\n",
      "     stat_ad_creative_id_s__view_7d = 43658681.0\n",
      "     stat_ad_creative_id_s__view_15d = 96734983.0\n",
      "     stat_ad_app_s__ctr_7d = 0.2\n",
      "     ad_info__budget_unit = 120\n",
      "预计batch总数: 2017.5244140625\n",
      "模型相关信息保存路径:  /home/zhoutong/tf_modelInfo/type=midas/dt=2018-12-08-18-29-39\n",
      "模型路径不存在，已创建新文件夹\n"
     ]
    }
   ],
   "source": [
    "# 输出一下参数\n",
    "import json\n",
    "with open(config_midas.info_file,\"r+\") as f:\n",
    "    info = f.read()\n",
    "    result = json.loads(info)\n",
    "for key,value in result.items():\n",
    "    print(key,\"--\")\n",
    "    for key_,value_ in value.items():\n",
    "        print(\"    \",key_,\"=\",value_)\n",
    "print(\"预计batch总数:\",(result['statistic']['train_pos']+result['statistic']['train_neg'])/config_midas.deepfm_param_dicts['batch_size'])\n",
    "print(\"模型相关信息保存路径: \",config_midas.base_save_dir)\n",
    "if not os.path.exists(config_midas.base_save_dir):\n",
    "    os.mkdir(config_midas.base_save_dir)\n",
    "    print(\"模型路径不存在，已创建新文件夹\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log工具 同时输出到文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T10:29:39.700596Z",
     "start_time": "2018-12-08T10:29:39.694813Z"
    },
    "code_folding": [
     4
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "logger = logging.getLogger()\n",
    "def setup_file_logger(log_file):\n",
    "    hdlr = logging.FileHandler(log_file)\n",
    "    formatter = logging.Formatter('%(levelname)s %(message)s')\n",
    "    hdlr.setFormatter(formatter)\n",
    "    logger.addHandler(hdlr) \n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "def myprint(message):\n",
    "    new_m = \"|{}| {}\".format(datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S'),message)\n",
    "    print(new_m)\n",
    "    logger.info(new_m)\n",
    "    \n",
    "setup_file_logger(config_midas.base_save_dir+\"/auc_logloss.log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre | TFRecord处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T10:29:39.713505Z",
     "start_time": "2018-12-08T10:29:39.703166Z"
    },
    "code_folding": [
     1
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "# ******** TFRecord - Dataset 读取**********\n",
    "def get_iterator(tfrecord_path,global_all_fields,global_multi_hot_fields,global_numeric_fields,max_numeric,tmp_map_num_f,batch_size):\n",
    "    # 解析TFRecord Example\n",
    "    def _decode(serialized_example):\n",
    "        feature_structure = {}\n",
    "        for field in global_all_fields:\n",
    "            if field == \"label\":\n",
    "                feature_structure[field]=tf.FixedLenFeature([], dtype=tf.int64)\n",
    "            elif field in global_multi_hot_fields:\n",
    "                feature_structure[field] = tf.VarLenFeature(dtype=tf.int64)\n",
    "            elif field in global_numeric_fields:\n",
    "                feature_structure[field] = tf.FixedLenFeature([],dtype=tf.float32)\n",
    "            else:\n",
    "                feature_structure[field]=tf.FixedLenFeature([], dtype=tf.int64)\n",
    "        parsed_features = tf.parse_single_example(serialized_example, feature_structure)\n",
    "        return parsed_features\n",
    "    # 连续特征归一化 | 考虑特征不会出现负数，如果最大值就是0那么这个特征全为0，归一化就直接取0\n",
    "    def _normalize(parsed_features):\n",
    "        for num_f in global_numeric_fields:\n",
    "            max_v = max_numeric[num_f]\n",
    "            parsed_features[num_f] = parsed_features[num_f] / max_v - 0.5 if max_v!=0 else 0\n",
    "        return parsed_features\n",
    "    # 把连续特征的idx加进去，跟样本一起出现batch_size次\n",
    "    def _add_idx_of_numeric(parsed_features):\n",
    "        for field in global_numeric_fields:\n",
    "            parsed_features[field+\"_idx\"] = tf.cast(tmp_map_num_f[field], tf.int64)\n",
    "        return parsed_features\n",
    "    # map并构造iterator\n",
    "    with tf.name_scope(\"dataset\"):\n",
    "        dataset = tf.data.TFRecordDataset(tfrecord_path,compression_type = \"GZIP\")\n",
    "        dataset = (dataset.map(_decode)\n",
    "                   .map(_normalize)\n",
    "                   .map(_add_idx_of_numeric))\n",
    "        dataset = (dataset.shuffle(5*batch_size)\n",
    "                   .batch(batch_size,drop_remainder=True))\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "    return iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre | DeepFM类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T10:29:39.800577Z",
     "start_time": "2018-12-08T10:29:39.716382Z"
    },
    "code_folding": [
     0
    ],
    "deletable": false,
    "editable": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "class DeepFM(object):\n",
    "    def __init__(self,train_tfrecord_file,valid_tfrecord_file,\n",
    "                 random_seed,base_save_dir,deepfm_param_dicts,data_param_dicts):\n",
    "        # 普通参数\n",
    "        self.random_seed = random_seed\n",
    "        tagTime= time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime(time.time()))\n",
    "        self.model_save_dir = base_save_dir+\"/model\"\n",
    "        self.summary_save_dir = base_save_dir+\"/summary\"\n",
    "        # TFRecord路径\n",
    "        self.train_tfrecord_file = train_tfrecord_file\n",
    "        self.valid_tfrecord_file = valid_tfrecord_file\n",
    "        # fields\n",
    "        self.global_all_fields = data_param_dicts['global_all_fields']\n",
    "        self.global_multi_hot_fields = data_param_dicts['global_multi_hot_fields']\n",
    "        self.global_numeric_fields = data_param_dicts['global_numeric_fields']\n",
    "        self.global_one_hot_fields = []\n",
    "        for i in self.global_all_fields:\n",
    "            if i not in self.global_numeric_fields and i not in self.global_multi_hot_fields and i != \"label\":\n",
    "                self.global_one_hot_fields.append(i)\n",
    "        self.max_numeric = data_param_dicts['max_numeric']\n",
    "        self.tmp_map_num_f = data_param_dicts['tmp_map_num_f']\n",
    "        self.numeric_field_size  = data_param_dicts['numeric_field_size']\n",
    "        self.one_hot_field_size = len(self.global_one_hot_fields)\n",
    "        self.multi_hot_field_size = data_param_dicts['multi_hot_field_size']\n",
    "\n",
    "        # deepfm 参数\n",
    "        self.dropout_fm = deepfm_param_dicts['dropout_fm']\n",
    "        self.dropout_deep = deepfm_param_dicts['dropout_deep']\n",
    "        self.feature_size = deepfm_param_dicts['feature_size']\n",
    "        self.batch_size = deepfm_param_dicts['batch_size']\n",
    "        self.epoch = deepfm_param_dicts['epoch']\n",
    "        self.embedding_size = deepfm_param_dicts['embedding_size']\n",
    "        self.deep_layers_activation = deepfm_param_dicts['deep_layers_activation']\n",
    "        self.batch_norm_decay = deepfm_param_dicts['batch_norm_decay']\n",
    "        self.deep_layers = deepfm_param_dicts['deep_layers']\n",
    "        self.learning_rate = deepfm_param_dicts['learning_rate']\n",
    "        self.l2_reg = deepfm_param_dicts['l2_reg']\n",
    "        # 初始化的变量\n",
    "        self.global_dense_shape = [self.batch_size,self.feature_size]\n",
    "        tf.set_random_seed(self.random_seed)\n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "        # graph returned\n",
    "        self.inp_tfrecord_path,self.inp_iterator,self.optimize_op,self.inputs_dict,self.outputs_dict,self.weights,self.ori_feed_dict,self.loss_op = self._init_graph()\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            self.train_phase = self.inputs_dict[\"train_phase\"]\n",
    "            self.label_op = self.inputs_dict['label']\n",
    "            self.pred = self.outputs_dict['pred']\n",
    "\n",
    "            self.merge_summary = tf.summary.merge_all()#调用sess.run运行图，生成一步的训练过程数据, 是一个option\n",
    "            self.writer = tf.summary.FileWriter(self.summary_save_dir, self.graph)\n",
    "\n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            # 注意如果不指定graph会使用默认graph，就获取不到在自定义的graph上的变量，报错 no variable to save\n",
    "            self.mySaver = tf.train.Saver(max_to_keep=2)\n",
    "    # ******** 初始化权重 ***********\n",
    "    def _initialize_weights(self):\n",
    "            multi_hot_field_size = self.multi_hot_field_size\n",
    "            one_hot_field_size = self.one_hot_field_size\n",
    "            numeric_field_size = self.numeric_field_size\n",
    "            feature_size = self.feature_size\n",
    "            embedding_size = self.embedding_size\n",
    "            deep_layers = self.deep_layers\n",
    "\n",
    "            weights = dict()\n",
    "            # embeddings\n",
    "            weights[\"feature_embeddings\"] = tf.Variable(\n",
    "                tf.random_normal([feature_size, embedding_size], -0.01, 0.01),\n",
    "                name=\"feature_embeddings\")  # feature_size * K\n",
    "            # FM first-order weights\n",
    "            weights[\"feature_bias\"] = tf.Variable(\n",
    "                tf.random_uniform([feature_size, 1], -0.01, 0.01), name=\"feature_bias\")  # feature_size * 1\n",
    "            # deep layers\n",
    "            # 总输入元个数为 : (涉及emb的特征个数) * embedding_size + 连续特征个数\n",
    "            input_size_emb = (multi_hot_field_size+one_hot_field_size) * embedding_size + numeric_field_size\n",
    "            glorot = np.sqrt(2.0 / (input_size_emb + deep_layers[0]))\n",
    "            weights[\"layer_0\"] = tf.Variable(\n",
    "                initial_value=np.random.normal(loc=0, scale=glorot, size=(input_size_emb, deep_layers[0])),\n",
    "                dtype=np.float32,\n",
    "                name=\"w_layer_0\")\n",
    "            weights[\"bias_0\"] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, deep_layers[0])),\n",
    "                                            dtype=np.float32, name=\"b_layer_0\")  # 1 * layers[0]\n",
    "            for i in range(1, len(deep_layers)):\n",
    "                glorot = np.sqrt(2.0 / (deep_layers[i - 1] + deep_layers[i]))\n",
    "                weights[\"layer_%d\" % i] = tf.Variable(\n",
    "                    np.random.normal(loc=0, scale=glorot, size=(deep_layers[i - 1], deep_layers[i])),\n",
    "                    dtype=np.float32, name=\"w_layer_%d\" % i)  # layers[i-1] * layers[i]\n",
    "                weights[\"bias_%d\" % i] = tf.Variable(\n",
    "                    np.random.normal(loc=0, scale=glorot, size=(1, deep_layers[i])),\n",
    "                    dtype=np.float32, name=\"b_layer_%d\" % i)  # 1 * layer[i]\n",
    "            # final concat projection layer\n",
    "            ################\n",
    "            # fm的y_first_order已经被提前求和了，所以只需要给它一个权重\n",
    "            # （因为在weights[\"feature_bias\"]中已经有部分作为“权重”乘上了y_first_order的特征值，然后求和，相当于每个一阶特征都有自己的隐向量x权重(来自w[\"feature_bias\"])\n",
    "            ################\n",
    "            cocnat_input_size_emb = 1 + embedding_size + deep_layers[-1]\n",
    "            glorot = np.sqrt(2.0 / (cocnat_input_size_emb + 1))\n",
    "            weights[\"concat_projection\"] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(cocnat_input_size_emb, 1)),\n",
    "                dtype=np.float32, name=\"concat_projection\")  # layers[i-1]*layers[i]\n",
    "            weights[\"concat_bias\"] = tf.Variable(tf.constant(0.01), dtype=np.float32, name=\"concat_bias\")\n",
    "            return weights\n",
    "\n",
    "\n",
    "    # ******** deepfm ***********\n",
    "    def _deep_fm_graph(self,weights, feat_total_idx_sp, feat_total_value_sp,\n",
    "                          feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list,\n",
    "                          feat_numeric_sp, feat_category_sp, train_phase):\n",
    "            def batch_norm_layer(x, inp_train_phase, scope_bn,inp_batch_norm_decay):\n",
    "                bn_train = batch_norm(x, decay=inp_batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                                      is_training=True, reuse=None, trainable=True, scope=scope_bn)\n",
    "                bn_inference = batch_norm(x, decay=inp_batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                                          is_training=False, reuse=True, trainable=True, scope=scope_bn)\n",
    "                z = tf.cond(inp_train_phase, lambda: bn_train, lambda: bn_inference)\n",
    "                return z\n",
    "\n",
    "            dropout_keep_fm = self.dropout_fm\n",
    "            dropout_keep_deep = self.dropout_deep\n",
    "            numeric_feature_size = self.numeric_field_size\n",
    "            onehot_field_size = self.one_hot_field_size\n",
    "            multi_hot_field_size = self.multi_hot_field_size\n",
    "            embedding_size = self.embedding_size\n",
    "            deep_layers_activation = self.deep_layers_activation\n",
    "            batch_norm_decay = self.batch_norm_decay\n",
    "            deep_input_size = multi_hot_field_size + onehot_field_size\n",
    "            # ---------- FM component ---------\n",
    "            with tf.name_scope(\"FM\"):\n",
    "                # ---------- first order term ----------\n",
    "                with tf.name_scope(\"1st_order\"):\n",
    "                    y_first_order = tf.nn.embedding_lookup_sparse(\n",
    "                        weights[\"feature_bias\"],\n",
    "                        sp_ids=feat_total_idx_sp,\n",
    "                        sp_weights=feat_total_value_sp,\n",
    "                        combiner=\"sum\")\n",
    "                    y_first_order = tf.nn.dropout(\n",
    "                        y_first_order,\n",
    "                        dropout_keep_fm[0],\n",
    "                        name=\"y_first_order_dropout\")\n",
    "                # ---------- second order term ---------------\n",
    "                with tf.name_scope(\"2nd_order\"):\n",
    "                    # sum_square part\n",
    "                    summed_features_emb_square = tf.square(\n",
    "                        tf.nn.embedding_lookup_sparse(\n",
    "                            weights[\"feature_embeddings\"],\n",
    "                            sp_ids=feat_total_idx_sp,\n",
    "                            sp_weights=feat_total_value_sp,\n",
    "                            combiner=\"sum\"))\n",
    "                    # square_sum part\n",
    "                    squared_sum_features_emb = tf.nn.embedding_lookup_sparse(\n",
    "                        tf.square(weights[\"feature_embeddings\"]),\n",
    "                        sp_ids=feat_total_idx_sp,\n",
    "                        sp_weights=tf.square(feat_total_value_sp),\n",
    "                        combiner=\"sum\")\n",
    "                    # second order\n",
    "                    y_second_order = 0.5 * tf.subtract(\n",
    "                        summed_features_emb_square,\n",
    "                        squared_sum_features_emb)  # None * K\n",
    "                    y_second_order = tf.nn.dropout(y_second_order,\n",
    "                                                   dropout_keep_fm[1])  # None * K\n",
    "            # ---------- Deep component -------\n",
    "            with tf.name_scope(\"Deep\"):\n",
    "                # total_embedding 均值 用户的multi-hot one-hot特征都取到embedding作为DNN输入\n",
    "                with tf.name_scope(\"total_emb\"):\n",
    "                    # feat_one_hot = tf.sparse_add(feat_numeric_sp, feat_category_sp)\n",
    "                    feat_one_hot = feat_category_sp\n",
    "                    one_hot_embeddings = tf.nn.embedding_lookup(\n",
    "                        weights[\"feature_embeddings\"], feat_one_hot.indices[:, 1])\n",
    "                    one_hot_embeddings = tf.reshape(\n",
    "                        one_hot_embeddings,\n",
    "                        shape=(-1, onehot_field_size, embedding_size))\n",
    "                    multi_hot_embeddings = []\n",
    "                    for feat_idx_sp, feat_value_sp in zip(\n",
    "                            feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list):\n",
    "                        emb = tf.nn.embedding_lookup_sparse(\n",
    "                            weights[\"feature_embeddings\"],\n",
    "                            sp_ids=feat_idx_sp,\n",
    "                            sp_weights=feat_value_sp,\n",
    "                            combiner=\"mean\")\n",
    "                        emb = tf.reshape(emb, shape=[-1, 1, embedding_size])\n",
    "                        multi_hot_embeddings.append(emb)\n",
    "                    total_embeddings = tf.concat(\n",
    "                        values=[one_hot_embeddings] + multi_hot_embeddings, axis=1)\n",
    "                # input\n",
    "                with tf.name_scope(\"input\"):\n",
    "                    # 把连续特征不经过embedding直接输入到NN\n",
    "                    feat_numeric_sp_dense = tf.cast(\n",
    "                        tf.reshape(\n",
    "                            feat_numeric_sp.values, shape=(-1, numeric_feature_size)),\n",
    "                        tf.float32)\n",
    "                    y_deep_input = tf.reshape(\n",
    "                        total_embeddings,\n",
    "                        shape=[-1, deep_input_size * embedding_size])  # None * (F*K)\n",
    "                    y_deep_input = tf.concat([y_deep_input, feat_numeric_sp_dense],\n",
    "                                             axis=1)\n",
    "                    y_deep_input = tf.nn.dropout(y_deep_input, dropout_keep_deep[0])\n",
    "                # layer0\n",
    "                with tf.name_scope(\"layer0\"):\n",
    "                    y_deep_layer_0 = tf.add(\n",
    "                        tf.matmul(y_deep_input, weights[\"layer_0\"]), weights[\"bias_0\"])\n",
    "                    y_deep_layer_0 = batch_norm_layer(\n",
    "                        y_deep_layer_0, inp_train_phase=train_phase, scope_bn=\"bn_0\",inp_batch_norm_decay=batch_norm_decay)\n",
    "                    y_deep_layer_0 = deep_layers_activation(y_deep_layer_0)\n",
    "                    y_deep_layer_0 = tf.nn.dropout(y_deep_layer_0, dropout_keep_deep[1])\n",
    "                # layer1\n",
    "                with tf.name_scope(\"layer1\"):\n",
    "                    y_deep_layer_1 = tf.add(\n",
    "                        tf.matmul(y_deep_layer_0, weights[\"layer_1\"]),\n",
    "                        weights[\"bias_1\"])\n",
    "                    y_deep_layer_1 = batch_norm_layer(\n",
    "                        y_deep_layer_1, inp_train_phase=train_phase, scope_bn=\"bn_1\",inp_batch_norm_decay=batch_norm_decay)\n",
    "                    y_deep_layer_1 = deep_layers_activation(y_deep_layer_1)\n",
    "                    y_deep_layer_1 = tf.nn.dropout(y_deep_layer_1, dropout_keep_deep[2])\n",
    "            # ---------- DeepFM ---------------\n",
    "            with tf.name_scope(\"DeepFM\"):\n",
    "                concat_input = tf.concat(\n",
    "                    [y_first_order, y_second_order, y_deep_layer_1], axis=1)\n",
    "                out = tf.add(\n",
    "                    tf.matmul(concat_input, weights[\"concat_projection\"]),\n",
    "                    weights[\"concat_bias\"])\n",
    "\n",
    "            return tf.nn.sigmoid(out)\n",
    "\n",
    "\n",
    "    # ******** 构造input并触发deepfm计算 ***********\n",
    "    def run_deepfm(self,weights,inp_list,train_phase):\n",
    "        def __add_idx_to_tensor(inp_tensor):\n",
    "            idx = tf.range(tf.shape(inp_tensor)[0])\n",
    "            idx_2d = tf.reshape(idx,[-1,1])\n",
    "            idx_2d_full = tf.cast(tf.tile(idx_2d,[1,tf.shape(inp_tensor)[1]]),dtype=inp_tensor.dtype)\n",
    "            added = tf.concat([tf.reshape(idx_2d_full,[-1,1]),tf.reshape(inp_tensor,[-1,1])],axis=1)\n",
    "            return added\n",
    "\n",
    "        def _get_numeric_sp(inp_dict):\n",
    "            if len(self.global_numeric_fields) !=0:\n",
    "                idx_to_stack=[]\n",
    "                value_to_stack=[]\n",
    "                for field in self.global_numeric_fields:\n",
    "                    idx_to_stack.append(inp_dict[field+\"_idx\"])\n",
    "                    value_to_stack.append(inp_dict[field])\n",
    "                idx_dense = __add_idx_to_tensor(tf.transpose(tf.stack(idx_to_stack)))\n",
    "                value_dense = tf.reshape(tf.transpose(tf.stack(value_to_stack)),[-1])\n",
    "            else:\n",
    "                # 为了保持连贯性，没有连续特征会构造“一个”连续特征，全为0\n",
    "                idx_dense = tf.constant([[i,0] for i in range(self.batch_size)],dtype=tf.int64)\n",
    "                value_dense = tf.constant([0.0]*self.batch_size,dtype=tf.float32)\n",
    "            return tf.SparseTensor(indices=idx_dense, values=value_dense, dense_shape=[self.batch_size,self.numeric_field_size])\n",
    "\n",
    "        def _get_category_sp(inp_dict):\n",
    "            idx_dense = tf.constant([[0,0]],dtype=tf.int64)\n",
    "            value_dense = tf.constant([0.0],dtype=tf.float32)\n",
    "            if len(self.global_one_hot_fields) != 0:\n",
    "                idx_to_stack=[]\n",
    "                value_to_stack=[]\n",
    "                for field in self.global_one_hot_fields:\n",
    "                    idx_to_stack.append(inp_dict[field])\n",
    "                    value_to_stack.append(tf.ones_like(inp_dict[field],dtype=tf.float32))\n",
    "                    idx_dense = __add_idx_to_tensor(tf.transpose(tf.stack(idx_to_stack)))\n",
    "                    value_dense = tf.reshape(tf.transpose(tf.stack(value_to_stack)),[-1])\n",
    "            return tf.SparseTensor(indices=idx_dense, values=value_dense, dense_shape=self.global_dense_shape)\n",
    "\n",
    "        def _get_multi_hot_idx_list(inp_dict):\n",
    "            multi_hot_idx_list = []\n",
    "            if len(self.global_multi_hot_fields) != 0:\n",
    "                for field in self.global_multi_hot_fields:\n",
    "                    multi_hot_idx_list.append(inp_dict[field])\n",
    "            else:\n",
    "                multi_hot_idx_list.append(tf.SparseTensor(indices=[[0,0]], values=[0.0], dense_shape=self.global_dense_shape))\n",
    "            return multi_hot_idx_list\n",
    "\n",
    "        def _make_multi_hot_value_list(feat_idx_list):\n",
    "            multi_hot_value_list = []\n",
    "            if len(feat_idx_list) !=0:\n",
    "                multi_hot_value_list=[tf.SparseTensor(indices=sparse.indices,values=tf.ones_like(sparse.values,dtype=tf.float32),dense_shape=sparse.dense_shape) for sparse in feat_idx_list]\n",
    "            else:\n",
    "                multi_hot_value_list.append(tf.SparseTensor(indices=[[0,0]], values=[0.0], dense_shape=self.global_dense_shape))\n",
    "            return multi_hot_value_list\n",
    "\n",
    "        def _get_total_feature(inp_dict):\n",
    "            idx_to_stack = []\n",
    "            value_to_stack = []\n",
    "            # sparse_tensor来表示multi_hot\n",
    "            multi_hot_idx_sparse_list = []\n",
    "            for field in self.global_all_fields:\n",
    "                if field in self.global_multi_hot_fields:\n",
    "                    multi_hot_idx_sparse_list.append(inp_dict[field])\n",
    "                if field in self.global_numeric_fields:\n",
    "                    idx_to_stack.append(inp_dict[field+\"_idx\"])\n",
    "                    value_to_stack.append(inp_dict[field])\n",
    "                    pass\n",
    "                if field in self.global_one_hot_fields:\n",
    "                    idx_to_stack.append(inp_dict[field])\n",
    "                    value_to_stack.append(tf.ones_like(inp_dict[field],dtype=tf.float32))\n",
    "                    pass\n",
    "            # sparse_tensor的values中原来都是特征索引，替换成1.0\n",
    "            multi_hot_value_sparse_list = [tf.SparseTensor(indices=sparse.indices, values=tf.ones_like(sparse.values,dtype=tf.float32), dense_shape=sparse.dense_shape) for sparse in multi_hot_idx_sparse_list]\n",
    "            # idx sparse of numeric+onehot\n",
    "            idx_dense = tf.transpose(tf.stack(idx_to_stack))\n",
    "            idx_sparse = tf.contrib.layers.dense_to_sparse(tensor=idx_dense,eos_token=-1)\n",
    "            # value sparse of numeric+onehot\n",
    "            value_dense = tf.transpose(tf.stack(value_to_stack))\n",
    "            value_sparse = tf.contrib.layers.dense_to_sparse(tensor=value_dense,eos_token=-1)\n",
    "\n",
    "            total_idx_sparse = tf.sparse_concat(axis=1,sp_inputs=[idx_sparse]+ multi_hot_idx_sparse_list)\n",
    "            total_value_sparse = tf.sparse_concat(axis=1,sp_inputs=[value_sparse] + multi_hot_value_sparse_list)\n",
    "            return total_idx_sparse, total_value_sparse\n",
    "        with tf.name_scope(\"gen_feat_total\"):\n",
    "            feat_total_idx_sp,feat_total_value_sp = _get_total_feature(inp_list)\n",
    "        with tf.name_scope(\"gen_feat_multi_hot\"):\n",
    "            feat_multi_hot_idx_sp_list = _get_multi_hot_idx_list(inp_list)\n",
    "            feat_multi_hot_value_sp_list = _make_multi_hot_value_list(feat_multi_hot_idx_sp_list)\n",
    "        with tf.name_scope(\"gen_feat_numeric\"):\n",
    "            feat_numeric_sp = _get_numeric_sp(inp_list)\n",
    "        with tf.name_scope(\"gen_feat_category\"):\n",
    "            feat_category_sp = _get_category_sp(inp_list)\n",
    "\n",
    "        return self._deep_fm_graph(weights,feat_total_idx_sp, feat_total_value_sp,\n",
    "                          feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list,\n",
    "                          feat_numeric_sp, feat_category_sp, train_phase)\n",
    "\n",
    "    # ******** 初始化计算图 ***********\n",
    "    def _init_graph(self):\n",
    "        with self.graph.as_default():\n",
    "            weights = self._initialize_weights()\n",
    "            total_parameters = 0\n",
    "            for variable in weights.values():\n",
    "                shape = variable.get_shape()\n",
    "                variable_parameters = 1\n",
    "                for dim in shape:\n",
    "                    variable_parameters *= dim.value\n",
    "                total_parameters += variable_parameters\n",
    "            myprint(\"total_parameters cnt : %s\" % total_parameters)\n",
    "\n",
    "            inp_tfrecord_path = tf.placeholder(dtype=tf.string, name=\"tfrecord_path\")\n",
    "            inp_iterator = get_iterator(inp_tfrecord_path,self.global_all_fields,self.global_multi_hot_fields,self.global_numeric_fields,self.max_numeric,self.tmp_map_num_f,self.batch_size)\n",
    "            inp_next_dict = inp_iterator.get_next()\n",
    "            # prepare\n",
    "            # inp_next_dict     key: decode时使用的字符串，value: tensor\n",
    "            #                   目的: 这个是iterator的next(get_next)结果\n",
    "            #                   示例: key: 'stat_ad_creative_id_s__cvr_3d'\n",
    "            #                        value: <tf.Tensor 'IteratorGetNext:57' shape=(3072,) dtype=int32>\n",
    "            # placeholder_dict  key: decode时使用的字符串，value: placeholder\n",
    "            #                   目的：为了让后面的流程都使用placeholder进行,这样存储模型可以以这些placeholder为输入口\n",
    "            #                   示例: key: 'ad_info__ad_creative_id_s'\n",
    "            #                        value: <tf.Tensor 'input/ad_info__ad_creative_id_s:0' shape=<unknown> dtype=int64>,\n",
    "            # ori_feed_dict     key: placeholder        value: tensor\n",
    "            #                   目的：直接sess.run(ori_feed_dict)就可以得到后续流程需要的placeholder的feed_dict;\n",
    "            #                   示例: key: <tf.Tensor 'input/ad_info__ad_creative_id_s:0' shape=<unknown> dtype=int64>\n",
    "            #                        value: <tf.Tensor 'IteratorGetNext:1' shape=(3072,) dtype=int64>\n",
    "            # 构造placeholder输入，方便模型文件restore后的使用\n",
    "            # 这里实际上只是把 inp_next 这个“源字典”的 value 都用placeholder替换了，key未变\n",
    "            placeholder_dict = {}\n",
    "            with tf.name_scope(\"input\"):\n",
    "                # train_phase放到这里只是为了共享同一个name_scope\n",
    "                train_phase = tf.placeholder(dtype=tf.bool,name=\"train_phase\")\n",
    "                placeholder_dict[\"train_phase\"]=train_phase\n",
    "                for k,v in inp_next_dict.items():\n",
    "                    if k in self.global_multi_hot_fields:\n",
    "                        placeholder_dict[k]=tf.sparse_placeholder(dtype=tf.int64,name=k)\n",
    "                    elif k in self.global_numeric_fields:\n",
    "                        placeholder_dict[k]=tf.placeholder(dtype=tf.float32,name=k)\n",
    "                    else:\n",
    "                        placeholder_dict[k]=tf.placeholder(dtype=tf.int64,name=k)\n",
    "                # 构造一个feed_dict在训练的时候自动就用它，取placeholder为key，取“源字典”的value为value\n",
    "                ori_feed_dict = {placeholder_dict[k] : inp_next_dict[k] for k,v in inp_next_dict.items()}\n",
    "\n",
    "            # deepfm\n",
    "            deepfm_output = self.run_deepfm(weights,placeholder_dict,train_phase)\n",
    "            with tf.name_scope(\"output\"):\n",
    "                pred = tf.reshape(deepfm_output,[-1],name=\"pred\")\n",
    "            # label\n",
    "            label_op = placeholder_dict['label']\n",
    "\n",
    "            # loss\n",
    "            empirical_risk = tf.reduce_mean(tf.losses.log_loss(label_op, pred))\n",
    "            loss_op = empirical_risk\n",
    "            if self.l2_reg>0:\n",
    "                structural_risk = tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"concat_projection\"])\n",
    "                structural_risk += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"feature_embeddings\"])\n",
    "                structural_risk += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"feature_bias\"])\n",
    "                for i in range(len(self.deep_layers)):\n",
    "                    structural_risk += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"layer_%d\"%i])\n",
    "                tf.summary.scalar('structural_risk_L2',structural_risk)\n",
    "                loss_op = empirical_risk + structural_risk\n",
    "\n",
    "            # optimizer\n",
    "            _optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,epsilon=1e-8)\n",
    "            grad = _optimizer.compute_gradients(loss_op)\n",
    "            optimize_op = _optimizer.apply_gradients(grad)\n",
    "\n",
    "            # summary (tensorboard)\n",
    "            tf.summary.scalar('total_loss', loss_op)\n",
    "            tf.summary.scalar('empirical_risk_logloss',empirical_risk)\n",
    "            for g,v in grad:\n",
    "                if g is not None:\n",
    "                    _=tf.summary.histogram(v.op.name+\"/gradients\",g)\n",
    "            for v in tf.trainable_variables():\n",
    "                _=tf.summary.histogram(v.name.replace(\":0\",\"/value\"),v)\n",
    "\n",
    "            inputs_dict = placeholder_dict\n",
    "            outputs_dict = {\"pred\":pred}\n",
    "        return inp_tfrecord_path,inp_iterator,optimize_op,inputs_dict,outputs_dict,weights,ori_feed_dict,loss_op\n",
    "\n",
    "    def _evaluate(self,sess,valid_dict):\n",
    "        pred_deque,label_deque=deque(),deque()\n",
    "        batch_cnt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                valid_dict.update(sess.run(self.ori_feed_dict))\n",
    "                batch_cnt += 1\n",
    "                t1 = time.time()\n",
    "                pred_,label_ = sess.run([self.pred,self.label_op],valid_dict)\n",
    "                pred_deque.extend(pred_)\n",
    "                label_deque.extend(label_)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "                sys.stdout.flush()\n",
    "                break\n",
    "            delta_t = time.time() - t1\n",
    "            sys.stdout.write(\"    valid_batch_cnt: [{batch_cnt:0>3d}] [{delta_t:.2f}s/per]\\r\".format(batch_cnt=batch_cnt,delta_t=delta_t))\n",
    "            sys.stdout.flush()\n",
    "        pred_arr = np.array(pred_deque)\n",
    "        label_arr = np.array(label_deque)\n",
    "        auc = roc_auc_score(label_arr,pred_arr)\n",
    "        loss = log_loss(label_arr,pred_arr,eps=1e-7)\n",
    "        return loss,auc\n",
    "\n",
    "    def _simple_save(self,sess,path,inputs,outputs,global_batch_cnt,auc,use_simple_save = False):\n",
    "        print(\"save model at %s\" % path)\n",
    "        if use_simple_save:\n",
    "            tf.saved_model.simple_save(sess,path+\"/model_of_auc-{auc:.5f}\".format(auc=auc),inputs,outputs)\n",
    "        else:\n",
    "            self.mySaver.save(sess, path+\"/model.ckpt\", global_step=global_batch_cnt)\n",
    "        pass\n",
    "\n",
    "    def fit(self):\n",
    "        train_feed={self.train_phase:True, self.inp_tfrecord_path:self.train_tfrecord_file}\n",
    "        valid_feed={self.train_phase:False, self.inp_tfrecord_path:self.valid_tfrecord_file}\n",
    "        # 不适用self.sess,因为必须在with结构内触发保存模型才能存住variable\n",
    "        model_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        model_config.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(graph=self.graph,config=model_config)\n",
    "        with sess as sess:\n",
    "            sess.run(self.init_op)\n",
    "            global_auc,global_batch_cnt,batch_cnt,epoch_cnt =0,0,0,0\n",
    "            for epoch in range(self.epoch):\n",
    "                epoch_cnt += 1\n",
    "                batch_cnt = 0\n",
    "                sess.run(self.inp_iterator.initializer,train_feed)\n",
    "                t0=time.time()\n",
    "                while True:\n",
    "                    try:\n",
    "                        batch_cnt += 1\n",
    "                        global_batch_cnt += 1\n",
    "                        train_feed.update(sess.run(self.ori_feed_dict))\n",
    "                        run_ops=[self.optimize_op,self.loss_op,self.pred,self.label_op,self.merge_summary]\n",
    "                        run_result = sess.run(run_ops,train_feed)\n",
    "                        _,loss_,pred_,label_,merge_summary_ = run_result\n",
    "                        self.writer.add_summary(merge_summary_,global_batch_cnt)\n",
    "                        if batch_cnt % 100 == 0:\n",
    "                            auc = roc_auc_score(label_,pred_)\n",
    "                            batch_time = time.time()-t0\n",
    "                            myprint(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d}] logloss:[{loss_:.5f}] auc:[{auc:.5f}] [{batch_time:.1f}s]\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt,loss_=loss_,auc=auc,batch_time=batch_time))\n",
    "                            t0=time.time()\n",
    "                        # 存在严重缺陷，这里如果用valid初始化后，从1001batch开始都会从valid里面拿数据了\n",
    "            #             if batch_cnt % 1000 ==0:\n",
    "            #                 sess.run(inp_iterator.initializer,valid_dict)\n",
    "            #                 logloss,auc=_evaluate(sess,valid_feed)\n",
    "            #                 now = time.strftime(\"|%Y-%m-%d %H:%M:%S| \", time.localtime(time.time()))\n",
    "            #                 print(f\"{now} [e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d} valid] logloss:[{logloss:.5f}] auc:[{auc:.5f}]\")\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        break\n",
    "                myprint(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d}] epoch-done\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt))\n",
    "                sess.run(self.inp_iterator.initializer,valid_feed)\n",
    "                logloss,auc=self._evaluate(sess,valid_feed)\n",
    "                myprint(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d} valid] valid_logloss:[{logloss:.5f}] valid_auc:[{auc:.5f}]\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt,logloss=logloss,auc=auc))\n",
    "                if global_auc<auc:\n",
    "                    global_auc = auc\n",
    "                    myprint(\"logloss:[{logloss:.5f}] auc:[{auc:.5f}] global_batch_cnt:[{global_batch_cnt:0>4d}] gonna save model ...\".format(logloss=logloss,auc=auc,global_batch_cnt=global_batch_cnt))\n",
    "                    self._simple_save(sess,self.model_save_dir,self.inputs_dict,self.outputs_dict,global_batch_cnt,auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T12:02:39.734559Z",
     "start_time": "2018-12-07T12:02:39.657632Z"
    },
    "code_folding": [
     0,
     57,
     184,
     197,
     408,
     432
    ],
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "class DeepFM(object):\n",
    "    def __init__(self,train_tfrecord_file,valid_tfrecord_file,\n",
    "                 random_seed,base_save_dir,deepfm_param_dicts,data_param_dicts):\n",
    "        # 普通参数\n",
    "        self.random_seed = random_seed\n",
    "        self.model_save_dir = base_save_dir+\"/{}\".format(\"model\")\n",
    "        self.summary_save_dir = base_save_dir+\"/{}\".format(\"summary\")\n",
    "        # TFRecord路径\n",
    "        self.train_tfrecord_file = train_tfrecord_file\n",
    "        self.valid_tfrecord_file = valid_tfrecord_file\n",
    "        # fields\n",
    "        self.global_all_fields = data_param_dicts['global_all_fields']\n",
    "        self.global_multi_hot_fields = data_param_dicts['global_multi_hot_fields']\n",
    "        self.global_numeric_fields = data_param_dicts['global_numeric_fields']\n",
    "        self.global_one_hot_fields = []\n",
    "        for i in self.global_all_fields:\n",
    "            if i not in self.global_numeric_fields and i not in self.global_multi_hot_fields and i != \"label\":\n",
    "                self.global_one_hot_fields.append(i)\n",
    "        self.max_numeric = data_param_dicts['max_numeric']\n",
    "        self.tmp_map_num_f = data_param_dicts['tmp_map_num_f']\n",
    "        self.numeric_field_size  = data_param_dicts['numeric_field_size']\n",
    "        self.one_hot_field_size = len(self.global_one_hot_fields)\n",
    "        self.multi_hot_field_size = data_param_dicts['multi_hot_field_size']\n",
    "\n",
    "        # deepfm 参数\n",
    "        self.dropout_fm = deepfm_param_dicts['dropout_fm']\n",
    "        self.dropout_deep = deepfm_param_dicts['dropout_deep']\n",
    "        self.feature_size = deepfm_param_dicts['feature_size']\n",
    "        self.batch_size = deepfm_param_dicts['batch_size']\n",
    "        self.epoch = deepfm_param_dicts['epoch']\n",
    "        self.embedding_size = deepfm_param_dicts['embedding_size']\n",
    "        self.deep_layers_activation = deepfm_param_dicts['deep_layers_activation']\n",
    "        self.batch_norm_decay = deepfm_param_dicts['batch_norm_decay']\n",
    "        self.deep_layers = deepfm_param_dicts['deep_layers']\n",
    "        self.learning_rate = deepfm_param_dicts['learning_rate']\n",
    "        self.l2_reg = deepfm_param_dicts['l2_reg']\n",
    "        # 初始化的变量\n",
    "        self.global_dense_shape = [self.batch_size,self.feature_size]\n",
    "        tf.set_random_seed(self.random_seed)\n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "        # graph returned\n",
    "        self.inp_tfrecord_path,self.inp_iterator,self.optimize_op,self.inputs_dict,self.outputs_dict,self.weights,self.ori_feed_dict,self.loss_op = self._init_graph()\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            self.train_phase = self.inputs_dict[\"train_phase\"]\n",
    "            self.label_op = self.inputs_dict['label']\n",
    "            self.pred = self.outputs_dict['pred']\n",
    "            \n",
    "            self.merge_summary = tf.summary.merge_all()#调用sess.run运行图，生成一步的训练过程数据, 是一个option\n",
    "            self.writer = tf.summary.FileWriter(self.summary_save_dir, self.graph)\n",
    "        \n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            # 注意如果不指定graph会使用默认graph，就获取不到在自定义的graph上的变量，报错 no variable to save \n",
    "            self.mySaver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "    # ******** 初始化权重 ***********\n",
    "    def _initialize_weights(self):\n",
    "            multi_hot_field_size = self.multi_hot_field_size\n",
    "            one_hot_field_size = self.one_hot_field_size\n",
    "            numeric_field_size = self.numeric_field_size\n",
    "            feature_size = self.feature_size\n",
    "            embedding_size = self.embedding_size\n",
    "            deep_layers = self.deep_layers\n",
    "\n",
    "            weights = dict()\n",
    "            # embeddings\n",
    "            weights[\"feature_embeddings\"] = tf.Variable(\n",
    "                tf.random_normal([feature_size, embedding_size], -0.01, 0.01),\n",
    "                name=\"feature_embeddings\")  # feature_size * K\n",
    "            # FM first-order weights\n",
    "            weights[\"feature_bias\"] = tf.Variable(\n",
    "                tf.random_uniform([feature_size, 1], -0.01, 0.01), name=\"feature_bias\")  # feature_size * 1\n",
    "            # deep layers\n",
    "            # 总输入元个数为 : (涉及emb的特征个数) * embedding_size + 连续特征个数\n",
    "            input_size_emb = (multi_hot_field_size+one_hot_field_size) * embedding_size + numeric_field_size\n",
    "            glorot = np.sqrt(2.0 / (input_size_emb + deep_layers[0]))\n",
    "            weights[\"layer_0\"] = tf.Variable(\n",
    "                initial_value=np.random.normal(loc=0, scale=glorot, size=(input_size_emb, deep_layers[0])),\n",
    "                dtype=np.float32,\n",
    "                name=\"w_layer_0\")\n",
    "            weights[\"bias_0\"] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, deep_layers[0])),\n",
    "                                            dtype=np.float32, name=\"b_layer_0\")  # 1 * layers[0]\n",
    "            for i in range(1, len(deep_layers)):\n",
    "                glorot = np.sqrt(2.0 / (deep_layers[i - 1] + deep_layers[i]))\n",
    "                weights[\"layer_%d\" % i] = tf.Variable(\n",
    "                    np.random.normal(loc=0, scale=glorot, size=(deep_layers[i - 1], deep_layers[i])),\n",
    "                    dtype=np.float32, name=\"w_layer_%d\" % i)  # layers[i-1] * layers[i]\n",
    "                weights[\"bias_%d\" % i] = tf.Variable(\n",
    "                    np.random.normal(loc=0, scale=glorot, size=(1, deep_layers[i])),\n",
    "                    dtype=np.float32, name=\"b_layer_%d\" % i)  # 1 * layer[i]\n",
    "            # final concat projection layer\n",
    "            ################\n",
    "            # fm的y_first_order已经被提前求和了，所以只需要给它一个权重\n",
    "            # （因为在weights[\"feature_bias\"]中已经有部分作为“权重”乘上了y_first_order的特征值，然后求和，相当于每个一阶特征都有自己的隐向量x权重(来自w[\"feature_bias\"])\n",
    "            ################\n",
    "            cocnat_input_size_emb = 1 + embedding_size + deep_layers[-1]\n",
    "            glorot = np.sqrt(2.0 / (cocnat_input_size_emb + 1))\n",
    "            weights[\"concat_projection\"] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(cocnat_input_size_emb, 1)),\n",
    "                dtype=np.float32, name=\"concat_projection\")  # layers[i-1]*layers[i]\n",
    "            weights[\"concat_bias\"] = tf.Variable(tf.constant(0.01), dtype=np.float32, name=\"concat_bias\")\n",
    "            return weights\n",
    "\n",
    "\n",
    "    # ******** deepfm ***********\n",
    "    def _deep_fm_graph(self,weights, feat_total_idx_sp, feat_total_value_sp,\n",
    "                          feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list,\n",
    "                          feat_numeric_sp, feat_category_sp, train_phase):\n",
    "            def batch_norm_layer(x, inp_train_phase, scope_bn,inp_batch_norm_decay):\n",
    "                bn_train = batch_norm(x, decay=inp_batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                                      is_training=True, reuse=None, trainable=True, scope=scope_bn)\n",
    "                bn_inference = batch_norm(x, decay=inp_batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                                          is_training=False, reuse=True, trainable=True, scope=scope_bn)\n",
    "                z = tf.cond(inp_train_phase, lambda: bn_train, lambda: bn_inference)\n",
    "                return z\n",
    "\n",
    "            dropout_keep_fm = self.dropout_fm\n",
    "            dropout_keep_deep = self.dropout_deep\n",
    "            numeric_feature_size = self.numeric_field_size\n",
    "            onehot_field_size = self.one_hot_field_size\n",
    "            multi_hot_field_size = self.multi_hot_field_size\n",
    "            embedding_size = self.embedding_size\n",
    "            deep_layers_activation = self.deep_layers_activation\n",
    "            batch_norm_decay = self.batch_norm_decay\n",
    "            deep_input_size = multi_hot_field_size + onehot_field_size\n",
    "            # ---------- FM component ---------\n",
    "            with tf.name_scope(\"FM\"):\n",
    "                # ---------- first order term ----------\n",
    "                with tf.name_scope(\"1st_order\"):\n",
    "                    y_first_order = tf.nn.embedding_lookup_sparse(\n",
    "                        weights[\"feature_bias\"],\n",
    "                        sp_ids=feat_total_idx_sp,\n",
    "                        sp_weights=feat_total_value_sp,\n",
    "                        combiner=\"sum\")\n",
    "                    y_first_order = tf.nn.dropout(\n",
    "                        y_first_order,\n",
    "                        dropout_keep_fm[0],\n",
    "                        name=\"y_first_order_dropout\")\n",
    "                # ---------- second order term ---------------\n",
    "                with tf.name_scope(\"2nd_order\"):\n",
    "                    # sum_square part\n",
    "                    summed_features_emb_square = tf.square(\n",
    "                        tf.nn.embedding_lookup_sparse(\n",
    "                            weights[\"feature_embeddings\"],\n",
    "                            sp_ids=feat_total_idx_sp,\n",
    "                            sp_weights=feat_total_value_sp,\n",
    "                            combiner=\"sum\"))\n",
    "                    # square_sum part\n",
    "                    squared_sum_features_emb = tf.nn.embedding_lookup_sparse(\n",
    "                        tf.square(weights[\"feature_embeddings\"]),\n",
    "                        sp_ids=feat_total_idx_sp,\n",
    "                        sp_weights=tf.square(feat_total_value_sp),\n",
    "                        combiner=\"sum\")\n",
    "                    # second order\n",
    "                    y_second_order = 0.5 * tf.subtract(\n",
    "                        summed_features_emb_square,\n",
    "                        squared_sum_features_emb)  # None * K\n",
    "                    y_second_order = tf.nn.dropout(y_second_order,\n",
    "                                                   dropout_keep_fm[1])  # None * K\n",
    "            # ---------- Deep component -------\n",
    "            with tf.name_scope(\"Deep\"):\n",
    "                # total_embedding 均值 用户的multi-hot one-hot特征都取到embedding作为DNN输入\n",
    "                with tf.name_scope(\"total_emb\"):\n",
    "                    # feat_one_hot = tf.sparse_add(feat_numeric_sp, feat_category_sp)\n",
    "                    feat_one_hot = feat_category_sp\n",
    "                    one_hot_embeddings = tf.nn.embedding_lookup(\n",
    "                        weights[\"feature_embeddings\"], feat_one_hot.indices[:, 1])\n",
    "                    one_hot_embeddings = tf.reshape(\n",
    "                        one_hot_embeddings,\n",
    "                        shape=(-1, onehot_field_size, embedding_size))\n",
    "                    multi_hot_embeddings = []\n",
    "                    for feat_idx_sp, feat_value_sp in zip(\n",
    "                            feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list):\n",
    "                        emb = tf.nn.embedding_lookup_sparse(\n",
    "                            weights[\"feature_embeddings\"],\n",
    "                            sp_ids=feat_idx_sp,\n",
    "                            sp_weights=feat_value_sp,\n",
    "                            combiner=\"mean\")\n",
    "                        emb = tf.reshape(emb, shape=[-1, 1, embedding_size])\n",
    "                        multi_hot_embeddings.append(emb)\n",
    "                    total_embeddings = tf.concat(\n",
    "                        values=[one_hot_embeddings] + multi_hot_embeddings, axis=1)\n",
    "                # input\n",
    "                with tf.name_scope(\"input\"):\n",
    "                    # 把连续特征不经过embedding直接输入到NN\n",
    "                    feat_numeric_sp_dense = tf.cast(\n",
    "                        tf.reshape(\n",
    "                            feat_numeric_sp.values, shape=(-1, numeric_feature_size)),\n",
    "                        tf.float32)\n",
    "                    y_deep_input = tf.reshape(\n",
    "                        total_embeddings,\n",
    "                        shape=[-1, deep_input_size * embedding_size])  # None * (F*K)\n",
    "                    y_deep_input = tf.concat([y_deep_input, feat_numeric_sp_dense],\n",
    "                                             axis=1)\n",
    "                    y_deep_input = tf.nn.dropout(y_deep_input, dropout_keep_deep[0])\n",
    "                # layer0\n",
    "                with tf.name_scope(\"layer0\"):\n",
    "                    y_deep_layer_0 = tf.add(\n",
    "                        tf.matmul(y_deep_input, weights[\"layer_0\"]), weights[\"bias_0\"])\n",
    "                    y_deep_layer_0 = batch_norm_layer(\n",
    "                        y_deep_layer_0, inp_train_phase=train_phase, scope_bn=\"bn_0\",inp_batch_norm_decay=batch_norm_decay)\n",
    "                    y_deep_layer_0 = deep_layers_activation(y_deep_layer_0)\n",
    "                    y_deep_layer_0 = tf.nn.dropout(y_deep_layer_0, dropout_keep_deep[1])\n",
    "                # layer1\n",
    "                with tf.name_scope(\"layer1\"):\n",
    "                    y_deep_layer_1 = tf.add(\n",
    "                        tf.matmul(y_deep_layer_0, weights[\"layer_1\"]),\n",
    "                        weights[\"bias_1\"])\n",
    "                    y_deep_layer_1 = batch_norm_layer(\n",
    "                        y_deep_layer_1, inp_train_phase=train_phase, scope_bn=\"bn_1\",inp_batch_norm_decay=batch_norm_decay)\n",
    "                    y_deep_layer_1 = deep_layers_activation(y_deep_layer_1)\n",
    "                    y_deep_layer_1 = tf.nn.dropout(y_deep_layer_1, dropout_keep_deep[2])\n",
    "            # ---------- DeepFM ---------------\n",
    "            with tf.name_scope(\"DeepFM\"):\n",
    "                concat_input = tf.concat(\n",
    "                    [y_first_order, y_second_order, y_deep_layer_1], axis=1)\n",
    "                out = tf.add(\n",
    "                    tf.matmul(concat_input, weights[\"concat_projection\"]),\n",
    "                    weights[\"concat_bias\"])\n",
    "            concat_ = tf.concat([y_first_order,y_second_order],axis=1)\n",
    "            out_ = tf.reduce_sum(concat_,axis=1)\n",
    "            return tf.nn.sigmoid(out_)\n",
    "\n",
    "\n",
    "    # ******** 构造input并触发deepfm计算 ***********\n",
    "    def run_deepfm(self,weights,inp_list,train_phase):\n",
    "        def __add_idx_to_tensor(inp_tensor):\n",
    "            idx = tf.range(tf.shape(inp_tensor)[0])\n",
    "            idx_2d = tf.reshape(idx,[-1,1])\n",
    "            idx_2d_full = tf.cast(tf.tile(idx_2d,[1,tf.shape(inp_tensor)[1]]),dtype=inp_tensor.dtype)\n",
    "            added = tf.concat([tf.reshape(idx_2d_full,[-1,1]),tf.reshape(inp_tensor,[-1,1])],axis=1)\n",
    "            return added\n",
    "\n",
    "        def _get_numeric_sp(inp_dict):\n",
    "            if len(self.global_numeric_fields) !=0:\n",
    "                idx_to_stack=[]\n",
    "                value_to_stack=[]\n",
    "                for field in self.global_numeric_fields:\n",
    "                    idx_to_stack.append(inp_dict[field+\"_idx\"])\n",
    "                    value_to_stack.append(inp_dict[field])\n",
    "                idx_dense = __add_idx_to_tensor(tf.transpose(tf.stack(idx_to_stack)))\n",
    "                value_dense = tf.reshape(tf.transpose(tf.stack(value_to_stack)),[-1])\n",
    "            else:\n",
    "                # 为了保持连贯性，没有连续特征会构造“一个”连续特征，全为0\n",
    "                idx_dense = tf.constant([[i,0] for i in range(self.batch_size)],dtype=tf.int64)\n",
    "                value_dense = tf.constant([0.0]*self.batch_size,dtype=tf.float32)\n",
    "            return tf.SparseTensor(indices=idx_dense, values=value_dense, dense_shape=[self.batch_size,self.numeric_field_size])\n",
    "\n",
    "        def _get_category_sp(inp_dict):\n",
    "            idx_dense = tf.constant([[0,0]],dtype=tf.int64)\n",
    "            value_dense = tf.constant([0.0],dtype=tf.float32)\n",
    "            if len(self.global_one_hot_fields) != 0:\n",
    "                idx_to_stack=[]\n",
    "                value_to_stack=[]\n",
    "                for field in self.global_one_hot_fields:\n",
    "                    idx_to_stack.append(inp_dict[field])\n",
    "                    value_to_stack.append(tf.ones_like(inp_dict[field],dtype=tf.float32))\n",
    "                    idx_dense = __add_idx_to_tensor(tf.transpose(tf.stack(idx_to_stack)))\n",
    "                    value_dense = tf.reshape(tf.transpose(tf.stack(value_to_stack)),[-1])\n",
    "            return tf.SparseTensor(indices=idx_dense, values=value_dense, dense_shape=self.global_dense_shape)\n",
    "\n",
    "        def _get_multi_hot_idx_list(inp_dict):\n",
    "            multi_hot_idx_list = []\n",
    "            if len(self.global_multi_hot_fields) != 0:\n",
    "                for field in self.global_multi_hot_fields:\n",
    "                    multi_hot_idx_list.append(inp_dict[field])\n",
    "            else:\n",
    "                multi_hot_idx_list.append(tf.SparseTensor(indices=[[0,0]], values=[0.0], dense_shape=self.global_dense_shape))\n",
    "            return multi_hot_idx_list\n",
    "\n",
    "        def _make_multi_hot_value_list(feat_idx_list):\n",
    "            multi_hot_value_list = []\n",
    "            if len(feat_idx_list) !=0:\n",
    "                multi_hot_value_list=[tf.SparseTensor(indices=sparse.indices,values=tf.ones_like(sparse.values,dtype=tf.float32),dense_shape=sparse.dense_shape) for sparse in feat_idx_list]\n",
    "            else:\n",
    "                multi_hot_value_list.append(tf.SparseTensor(indices=[[0,0]], values=[0.0], dense_shape=self.global_dense_shape))\n",
    "            return multi_hot_value_list\n",
    "\n",
    "        def _get_total_feature(inp_dict):\n",
    "            idx_to_stack = []\n",
    "            value_to_stack = []\n",
    "            # sparse_tensor来表示multi_hot\n",
    "            multi_hot_idx_sparse_list = []\n",
    "            for field in self.global_all_fields:\n",
    "                if field in self.global_multi_hot_fields:\n",
    "                    multi_hot_idx_sparse_list.append(inp_dict[field])\n",
    "                if field in self.global_numeric_fields:\n",
    "                    idx_to_stack.append(inp_dict[field+\"_idx\"])\n",
    "                    value_to_stack.append(inp_dict[field])\n",
    "                    pass\n",
    "                if field in self.global_one_hot_fields:\n",
    "                    idx_to_stack.append(inp_dict[field])\n",
    "                    value_to_stack.append(tf.ones_like(inp_dict[field],dtype=tf.float32))\n",
    "                    pass\n",
    "            # sparse_tensor的values中原来都是特征索引，替换成1.0\n",
    "            multi_hot_value_sparse_list = [tf.SparseTensor(indices=sparse.indices, values=tf.ones_like(sparse.values,dtype=tf.float32), dense_shape=sparse.dense_shape) for sparse in multi_hot_idx_sparse_list]\n",
    "            # idx sparse of numeric+onehot\n",
    "            idx_dense = tf.transpose(tf.stack(idx_to_stack))\n",
    "            idx_sparse = tf.contrib.layers.dense_to_sparse(tensor=idx_dense,eos_token=-1)\n",
    "            # value sparse of numeric+onehot\n",
    "            value_dense = tf.transpose(tf.stack(value_to_stack))\n",
    "            value_sparse = tf.contrib.layers.dense_to_sparse(tensor=value_dense,eos_token=-1)\n",
    "\n",
    "            total_idx_sparse = tf.sparse_concat(axis=1,sp_inputs=[idx_sparse]+ multi_hot_idx_sparse_list)\n",
    "            total_value_sparse = tf.sparse_concat(axis=1,sp_inputs=[value_sparse] + multi_hot_value_sparse_list)\n",
    "            return total_idx_sparse, total_value_sparse\n",
    "        with tf.name_scope(\"gen_feat_total\"):\n",
    "            feat_total_idx_sp,feat_total_value_sp = _get_total_feature(inp_list)\n",
    "        with tf.name_scope(\"gen_feat_multi_hot\"):\n",
    "            feat_multi_hot_idx_sp_list = _get_multi_hot_idx_list(inp_list)\n",
    "            feat_multi_hot_value_sp_list = _make_multi_hot_value_list(feat_multi_hot_idx_sp_list)\n",
    "        with tf.name_scope(\"gen_feat_numeric\"):\n",
    "            feat_numeric_sp = _get_numeric_sp(inp_list)\n",
    "        with tf.name_scope(\"gen_feat_category\"):\n",
    "            feat_category_sp = _get_category_sp(inp_list)\n",
    "\n",
    "        return self._deep_fm_graph(weights,feat_total_idx_sp, feat_total_value_sp,\n",
    "                          feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list,\n",
    "                          feat_numeric_sp, feat_category_sp, train_phase)\n",
    "\n",
    "    # ******** 初始化计算图 ***********\n",
    "    def _init_graph(self):\n",
    "        with self.graph.as_default():\n",
    "            weights = self._initialize_weights()\n",
    "            total_parameters = 0\n",
    "            for variable in weights.values():\n",
    "                shape = variable.get_shape()\n",
    "                variable_parameters = 1\n",
    "                for dim in shape:\n",
    "                    variable_parameters *= dim.value\n",
    "                total_parameters += variable_parameters\n",
    "            log(\"total_parameters cnt : %s\" % total_parameters)\n",
    "#             for k,v in weights.items():\n",
    "#                 dim_list = [dim.value for dim in v.get_shape()]\n",
    "#                 reduce_prod_dim = reduce(lambda x, y: x*y, dim_list) if len(dim_list)>0 else 0\n",
    "#                 log(k+\" size=\"+\"*\".join([str(i) for i in dim_list])+\"=\" + str(reduce_prod_dim))\n",
    "\n",
    "            inp_tfrecord_path = tf.placeholder(dtype=tf.string, name=\"tfrecord_path\")\n",
    "            inp_iterator = get_iterator(inp_tfrecord_path,self.global_all_fields,self.global_multi_hot_fields,self.global_numeric_fields,self.max_numeric,self.tmp_map_num_f,self.batch_size)\n",
    "            inp_next_dict = inp_iterator.get_next()\n",
    "            # prepare\n",
    "            # inp_next_dict     key: decode时使用的字符串，value: tensor\n",
    "            # placeholder_dict  key: decode时使用的字符串，value: placeholder\n",
    "            #                   目的：为了让后面的流程都使用placeholder进行,这样存储模型可以以这些placeholder为输入口\n",
    "            #                   示例: 'ad_info__ad_creative_id_s':<tf.Tensor 'input/ad_info__ad_creative_id_s:0' shape=<unknown> dtype=int64>,\n",
    "            # ori_feed_dict     key: placeholder        value: tensor\n",
    "            #                   目的：直接sess.run(ori_feed_dict)就可以得到后续流程需要的placeholder的feed_dict;\n",
    "            # 构造placeholder输入，方便模型文件restore后的使用\n",
    "            # 这里实际上只是把 inp_next 这个“源字典”的 value 都用placeholder替换了，key未变\n",
    "            placeholder_dict = {}\n",
    "            with tf.name_scope(\"input\"):\n",
    "                # train_phase放到这里只是为了共享同一个name_scope\n",
    "                train_phase = tf.placeholder(dtype=tf.bool,name=\"train_phase\")\n",
    "                placeholder_dict[\"train_phase\"]=train_phase\n",
    "                for k,v in inp_next_dict.items():\n",
    "                    if k in self.global_multi_hot_fields:\n",
    "                        placeholder_dict[k]=tf.sparse_placeholder(dtype=tf.int64,name=k)\n",
    "                    elif k in self.global_numeric_fields:\n",
    "                        placeholder_dict[k]=tf.placeholder(dtype=tf.float32,name=k)\n",
    "                    else:\n",
    "                        placeholder_dict[k]=tf.placeholder(dtype=tf.int64,name=k)\n",
    "                # 构造一个feed_dict在训练的时候自动就用它，取placeholder为key，取“源字典”的value为value\n",
    "                ori_feed_dict = {placeholder_dict[k] : inp_next_dict[k] for k,v in inp_next_dict.items()}\n",
    "\n",
    "            # deepfm\n",
    "            deepfm_output = self.run_deepfm(weights,placeholder_dict,train_phase)\n",
    "            with tf.name_scope(\"output\"):\n",
    "                pred = tf.reshape(deepfm_output,[-1],name=\"pred\")\n",
    "            # label\n",
    "            label_op = placeholder_dict['label']\n",
    "\n",
    "            # loss\n",
    "            empirical_risk = tf.reduce_mean(tf.losses.log_loss(label_op, pred))\n",
    "            loss_op = empirical_risk\n",
    "            if self.l2_reg>0:\n",
    "#                 structural_risk = tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"concat_projection\"])\n",
    "                structural_risk = tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"feature_embeddings\"])\n",
    "                structural_risk += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"feature_bias\"])\n",
    "#                 for i in range(len(self.deep_layers)):\n",
    "#                     structural_risk += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"layer_%d\"%i])\n",
    "                tf.summary.scalar('structural_risk_L2',structural_risk)\n",
    "                loss_op = empirical_risk + structural_risk\n",
    "\n",
    "            # loss_op = tf.reduce_mean(tf.losses.log_loss(label_op, pred))\n",
    "#             if self.l2_reg>0:\n",
    "#                 loss_op += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"concat_projection\"])\n",
    "#                 for i in range(len(self.deep_layers)):\n",
    "#                     loss_op += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"layer_%d\"%i])\n",
    "\n",
    "            # optimizer\n",
    "            _optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,epsilon=1e-8)\n",
    "            grad = _optimizer.compute_gradients(loss_op)\n",
    "            optimize_op = _optimizer.apply_gradients(grad)\n",
    "\n",
    "            # summary (tensorboard)\n",
    "            tf.summary.scalar('total_loss', loss_op)\n",
    "            tf.summary.scalar('empirical_risk_logloss',empirical_risk)\n",
    "            for g,v in grad:\n",
    "                if g is not None:\n",
    "                    _=tf.summary.histogram(v.op.name+\"/gradients\",g)\n",
    "            for v in tf.trainable_variables():\n",
    "                _=tf.summary.histogram(v.name.replace(\":0\",\"/value\"),v)\n",
    "            \n",
    "            inputs_dict = placeholder_dict\n",
    "            outputs_dict = {\"pred\":pred}\n",
    "        return inp_tfrecord_path,inp_iterator,optimize_op,inputs_dict,outputs_dict,weights,ori_feed_dict,loss_op\n",
    "\n",
    "    def _evaluate(self,sess,valid_dict):\n",
    "        pred_deque,label_deque=deque(),deque()\n",
    "        batch_cnt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                valid_dict.update(sess.run(self.ori_feed_dict))\n",
    "                batch_cnt += 1\n",
    "                t1 = time.time()\n",
    "                pred_,label_ = sess.run([self.pred,self.label_op],valid_dict)\n",
    "                pred_deque.extend(pred_)\n",
    "                label_deque.extend(label_)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "                sys.stdout.flush()\n",
    "                break\n",
    "            delta_t = time.time() - t1\n",
    "            sys.stdout.write(\"    valid_batch_cnt: [{batch_cnt:0>3d}] [{delta_t:.2f}s/per]\\r\".format(batch_cnt=batch_cnt,delta_t=delta_t))\n",
    "            sys.stdout.flush()\n",
    "        pred_arr = np.array(pred_deque)\n",
    "        label_arr = np.array(label_deque)\n",
    "        auc = roc_auc_score(label_arr,pred_arr)\n",
    "        loss = log_loss(label_arr,pred_arr,eps=1e-7)\n",
    "        return loss,auc\n",
    "\n",
    "    def _simple_save(self,sess,path,inputs,outputs,global_batch_cnt,auc,use_simple_save = False):\n",
    "        print(\"save model at %s\" % path)\n",
    "        if use_simple_save:\n",
    "            tf.saved_model.simple_save(sess,path+\"/model_of_auc-{auc:.5f}\".format(auc=auc),inputs,outputs)\n",
    "        else:\n",
    "            self.mySaver.save(sess, path+\"/model.ckpt\", global_step=global_batch_cnt)\n",
    "        pass\n",
    "\n",
    "    def fit(self):\n",
    "        train_feed={self.train_phase:True, self.inp_tfrecord_path:self.train_tfrecord_file}\n",
    "        valid_feed={self.train_phase:False, self.inp_tfrecord_path:self.valid_tfrecord_file}\n",
    "        # 不适用self.sess,因为必须在with结构内触发保存模型才能存住variable\n",
    "        # 如果使用 with self.sess as sess，则with结束后self.sess直接销毁了，没有意义\n",
    "        model_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        model_config.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(graph=self.graph,config=model_config)\n",
    "        with sess as sess:\n",
    "            sess.run(self.init_op)\n",
    "            global_auc,global_batch_cnt,batch_cnt,epoch_cnt =0,0,0,0\n",
    "            for epoch in range(self.epoch):\n",
    "                epoch_cnt += 1\n",
    "                batch_cnt = 0\n",
    "                sess.run(self.inp_iterator.initializer,train_feed)\n",
    "                t0=time.time()\n",
    "                while True:\n",
    "                    try:\n",
    "                        batch_cnt += 1\n",
    "                        global_batch_cnt += 1\n",
    "                        # 加上代表placeholder和其value的键值 | train_feed目前只有train_phase和tfrecord_path这些外部参数\n",
    "                        train_feed.update(sess.run(self.ori_feed_dict))\n",
    "                        run_ops=[self.optimize_op,self.loss_op,self.pred,self.label_op,self.merge_summary]\n",
    "                        run_result = sess.run(run_ops,train_feed)\n",
    "                        _,loss_,pred_,label_,merge_summary_ = run_result\n",
    "                        self.writer.add_summary(merge_summary_,global_batch_cnt)\n",
    "#                         if batch_cnt % 10 == 0:\n",
    "#                             log(sess.run(self.weights[\"feature_embeddings\"][0,:]))\n",
    "                        if batch_cnt % 100 == 0:\n",
    "                            auc = roc_auc_score(label_,pred_)\n",
    "                            batch_time = time.time()-t0\n",
    "                            log(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d}] logloss:[{loss_:.5f}] auc:[{auc:.5f}] [{batch_time:.1f}s]\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt,loss_=loss_,auc=auc,batch_time=batch_time))\n",
    "                            t0=time.time()\n",
    "                        # 存在严重缺陷导致不能在中途显示验证集auc logloss，\n",
    "                        # 这里如果用valid初始化iterator后，从1001batch开始都会从valid里面拿数据了\n",
    "            #             if batch_cnt % 1000 ==0:\n",
    "            #                 sess.run(inp_iterator.initializer,valid_dict)\n",
    "            #                 logloss,auc=_evaluate(sess,valid_feed)\n",
    "            #                 now = time.strftime(\"|%Y-%m-%d %H:%M:%S| \", time.localtime(time.time()))\n",
    "            #                 print(f\"{now} [e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d} valid] logloss:[{logloss:.5f}] auc:[{auc:.5f}]\")\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        break\n",
    "                log(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d}] epoch-done\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt))\n",
    "                sess.run(self.inp_iterator.initializer,valid_feed)\n",
    "                logloss,auc=self._evaluate(sess,valid_feed)\n",
    "                log(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d} valid] valid_logloss:[{logloss:.5f}] valid_auc:[{auc:.5f}]\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt,logloss=logloss,auc=auc))\n",
    "                if global_auc<auc:\n",
    "                    global_auc = auc\n",
    "                    log(\"logloss:[{logloss:.5f}] auc:[{auc:.5f}] global_batch_cnt:[{global_batch_cnt:0>4d}] gonna save model ...\".format(logloss=logloss,auc=auc,global_batch_cnt=global_batch_cnt))\n",
    "                    self._simple_save(sess,self.model_save_dir,self.inputs_dict,self.outputs_dict,global_batch_cnt,auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T10:29:45.750555Z",
     "start_time": "2018-12-08T10:29:39.802726Z"
    },
    "init_cell": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|2018-12-08 18:29:39| total_parameters cnt : 6000414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhoutong/python3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:108: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "process = DeepFM(CONFIG.train_tfrecord_file,CONFIG.valid_tfrecord_file,CONFIG.random_seed,CONFIG.base_save_dir,CONFIG.deepfm_param_dicts,CONFIG.data_param_dicts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-08T10:30:25.637Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "process.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-07T11:59:59.990463Z",
     "start_time": "2018-12-07T11:59:59.240601Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights=process.weights\n",
    "model_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "model_config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(graph=process.graph,config=model_config)\n",
    "sess.run(process.init_op)\n",
    "print(sess.run(weights[\"feature_embeddings\"][0,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:31:23.961414Z",
     "start_time": "2018-11-05T09:31:23.958081Z"
    }
   },
   "source": [
    "## Inference |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-05T09:31:33.339685Z",
     "start_time": "2018-11-05T09:31:33.318455Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# ******** 从磁盘解析TFRecord数据进行预测 **********\n",
    "class Inference(object):\n",
    "    def __init__(self,model_path,model_type=\"pb\",out_tensor_name=\"output/pred:0\",inp_tensor_prefix=\"input\"):\n",
    "        # params\n",
    "        self.model_p = model_path\n",
    "        self.out_tensor_name= out_tensor_name\n",
    "        self.inp_tensor_prefix = inp_tensor_prefix\n",
    "        # graph & sess\n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "        with self.graph.as_default():\n",
    "            model_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "            model_config.gpu_options.allow_growth = True\n",
    "            self.sess = tf.Session(graph=self.graph,config=model_config)\n",
    "            # restore\n",
    "            if model_type==\"pb\":\n",
    "                _ = tf.saved_model.loader.load(self.sess,[tag_constants.SERVING],self.model_p)\n",
    "            elif model_type==\"ckpt\":\n",
    "                saver = tf.train.import_meta_graph(self.model_p+\".meta\")\n",
    "                saver.restore(self.sess, self.model_p)\n",
    "            else:\n",
    "                assert False, \"model_type should be either 'pb' or 'ckpt'\"\n",
    "            init_op = tf.global_variables_initializer()\n",
    "        # init\n",
    "        self.sess.run(init_op)\n",
    "        # prepare input & output\n",
    "        self.pred = self.sess.graph.get_tensor_by_name(out_tensor_name)\n",
    "        self.to_feed_ph = []\n",
    "        for op in self.sess.graph.get_operations():\n",
    "            if op.name.startswith(self.inp_tensor_prefix) and \"label\" not in op.name :\n",
    "                ph = self.sess.graph.get_tensor_by_name(op.name+\":0\")\n",
    "                self.to_feed_ph.append(ph)\n",
    "        print(\"name of tensors(placeholder) to input:\")\n",
    "        for ph in self.to_feed_ph:\n",
    "            print(\"    \",ph.name)\n",
    "\n",
    "    def infer(self,inp_dict):\n",
    "        feed_dict = {ph:inp_dict[ph.name] for ph in self.to_feed_ph}\n",
    "        pred_ = self.sess.run(self.pred,feed_dict)\n",
    "        return pred_\n",
    "\n",
    "    @staticmethod\n",
    "    def infer_tfrecord_iterator(valid_iterator_inp):\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(valid_iterator_inp.initializer)\n",
    "            inp_next = valid_iterator_inp.get_next()\n",
    "            label_queue = deque()\n",
    "            pred_queue = deque()\n",
    "            while True:\n",
    "                try:\n",
    "                    inp_next_value = sess.run(inp_next)\n",
    "                    if 'label' in inp_next_value.keys:\n",
    "                        label_queue.extend(inp_next_value['label'])\n",
    "                    inp_dict = {}\n",
    "                    for k,v in inp_next_value.items():\n",
    "                        if k in load_config.global_multi_hot_fields:\n",
    "                            inp_dict[\"input/\"+k+\"/shape:0\"] = v.dense_shape\n",
    "                            inp_dict[\"input/\"+k+\"/values:0\"] = v.values\n",
    "                            inp_dict[\"input/\"+k+\"/indices:0\"] = v.indices\n",
    "                        else:\n",
    "                            inp_dict[\"input/\"+k+\":0\"] = v\n",
    "                    inp_dict[\"input/train_phase:0\"] = False\n",
    "                    pred_queue.extend(inferer.infer(inp_dict))\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "            return label_queue,pred_queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference | 读取TFRecord用的Config文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# ******* Inference读取TFRecord使用的config文件,包含基础的描述信息 ********\n",
    "class load_config(object):\n",
    "    basePath = \"/home/zhoutong/data/starksdk/tfrecord_2018-09-21_to_2018-10-04_and_2018-10-05_to_2018-10-11\"\n",
    "\n",
    "    train_tfrecord_file = basePath+\"/train.tfrecord.gz\"\n",
    "    valid_tfrecord_file = basePath+\"/valid.tfrecord.gz\"\n",
    "    info_file = basePath+\"/info.json\"\n",
    "    # fields\n",
    "    with open(info_file,\"r+\") as f:\n",
    "        info = \"\".join(f.readlines())\n",
    "        result = json.loads(info)\n",
    "\n",
    "    fieldInfo = result['allField']\n",
    "    global_all_fields = fieldInfo['all_fields'].split(\",\")\n",
    "    global_numeric_fields = [] if fieldInfo['numeric_fields'].split(\",\")==[''] else fieldInfo['numeric_fields'].split(\",\")\n",
    "    global_multi_hot_fields = [] if fieldInfo['multi_hot_fields'].split(\",\")==[''] else fieldInfo['multi_hot_fields'].split(\",\")\n",
    "    tmp_map_num_f = result['numericFieldMap']#{'ad_info__budget_unit':1291744}\n",
    "    max_numeric = result['numericMax']#{\"ad_info__budget_unit\": 2.0}\n",
    "    batch_size = 1024*6\n",
    "\n",
    "valid_iterator = get_iterator(load_config.valid_tfrecord_file,\n",
    "                              load_config.global_all_fields,\n",
    "                              load_config.global_multi_hot_fields,\n",
    "                              load_config.global_numeric_fields,\n",
    "                              load_config.max_numeric,\n",
    "                              load_config.tmp_map_num_f,\n",
    "                              load_config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_p = \"/home/zhoutong/starksdk_model/model_2018-10-31-20-10-32\"+\"/model.ckpt-1254\"\n",
    "inferer = Inference(model_p,model_type=\"ckpt\",out_tensor_name=\"output/pred:0\",inp_tensor_prefix=\"input\")\n",
    "label,pred = inferer.infer_tfrecord_iterator(valid_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX | 模型转成ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********* 模型(ckpt)转成 onnx *******\n",
    "model_p = \"/Users/zac/model_2018-11-01-12-13-25\"+\"/model.ckpt-1124\"\n",
    "onnx_path = \"/Users/zac/tmp/model.onnx\"\n",
    "def transform2onnx(model_path_inp, onnx_path_inp):\n",
    "    with tf.Session() as sess:\n",
    "        saver = tf.train.import_meta_graph(model_path_inp + \".meta\")\n",
    "        saver.restore(sess, model_path_inp)\n",
    "        onnx_graph = tf2onnx.tfonnx.process_tf_graph(sess.graph)\n",
    "        to_feed_ph = []\n",
    "        for op in sess.graph.get_operations():\n",
    "            if op.name.startswith(\"input\") and \"label\" not in op.name :\n",
    "                ph = sess.graph.get_tensor_by_name(op.name+\":0\")\n",
    "                to_feed_ph.append(ph)\n",
    "        model_proto = onnx_graph.make_model(\"test\", [ph.name for ph in to_feed_ph], [\"output/pred:0\"])\n",
    "        with open(onnx_path_inp, \"wb+\") as f:\n",
    "            f.write(model_proto.SerializeToString())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T03:53:48.626700Z",
     "start_time": "2018-12-04T03:53:48.513732Z"
    },
    "code_folding": [
     0,
     45,
     58,
     68,
     72,
     78,
     82,
     84,
     110,
     128,
     162,
     233,
     234,
     245,
     252,
     267,
     280,
     289,
     352,
     387,
     395,
     403,
     426,
     450,
     458
    ]
   },
   "outputs": [],
   "source": [
    "class DeepFM(object):\n",
    "    def __init__(self,train_tfrecord_file,valid_tfrecord_file,\n",
    "                 random_seed,base_save_dir,deepfm_param_dicts,data_param_dicts):\n",
    "        # 普通参数\n",
    "        self.random_seed = random_seed\n",
    "        tagTime= time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime(time.time()))\n",
    "        self.model_save_dir = base_save_dir+\"/dt=%s/%s\".format(tagTime,\"model\")\n",
    "        self.summary_save_dir = base_save_dir+\"/dt=%s/%s\".format(tagTime,\"summary\")\n",
    "        # TFRecord路径\n",
    "        self.train_tfrecord_file = train_tfrecord_file\n",
    "        self.valid_tfrecord_file = valid_tfrecord_file\n",
    "        # fields\n",
    "        self.global_all_fields = data_param_dicts['global_all_fields']\n",
    "        self.global_multi_hot_fields = data_param_dicts['global_multi_hot_fields']\n",
    "        self.global_numeric_fields = data_param_dicts['global_numeric_fields']\n",
    "        self.global_one_hot_fields = []\n",
    "        for i in self.global_all_fields:\n",
    "            if i not in self.global_numeric_fields and i not in self.global_multi_hot_fields and i != \"label\":\n",
    "                self.global_one_hot_fields.append(i)\n",
    "        self.max_numeric = data_param_dicts['max_numeric']\n",
    "        self.tmp_map_num_f = data_param_dicts['tmp_map_num_f']\n",
    "        self.numeric_field_size  = data_param_dicts['numeric_field_size']\n",
    "        self.one_hot_field_size = len(self.global_one_hot_fields)\n",
    "        self.multi_hot_field_size = data_param_dicts['multi_hot_field_size']\n",
    "\n",
    "        # deepfm 参数\n",
    "        self.dropout_fm = deepfm_param_dicts['dropout_fm']\n",
    "        self.dropout_deep = deepfm_param_dicts['dropout_deep']\n",
    "        self.feature_size = deepfm_param_dicts['feature_size']\n",
    "        self.batch_size = deepfm_param_dicts['batch_size']\n",
    "        self.epoch = deepfm_param_dicts['epoch']\n",
    "        self.embedding_size = deepfm_param_dicts['embedding_size']\n",
    "        self.deep_layers_activation = deepfm_param_dicts['deep_layers_activation']\n",
    "        self.batch_norm_decay = deepfm_param_dicts['batch_norm_decay']\n",
    "        self.deep_layers = deepfm_param_dicts['deep_layers']\n",
    "        self.learning_rate = deepfm_param_dicts['learning_rate']\n",
    "        self.l2_reg = deepfm_param_dicts['l2_reg']\n",
    "        # 初始化的变量\n",
    "        self.global_dense_shape = [self.batch_size,self.feature_size]\n",
    "        tf.set_random_seed(self.random_seed)\n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "        # graph returned\n",
    "        self.inp_tfrecord_path,self.inp_iterator,self.optimize_op,self.inputs_dict,self.outputs_dict,self.weights,self.ori_feed_dict,self.loss_op = self._init_graph()\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            self.train_phase = self.inputs_dict[\"train_phase\"]\n",
    "            self.label_op = self.inputs_dict['label']\n",
    "            self.pred = self.outputs_dict['pred']\n",
    "            \n",
    "            self.merge_summary = tf.summary.merge_all()#调用sess.run运行图，生成一步的训练过程数据, 是一个option\n",
    "            self.writer = tf.summary.FileWriter(self.summary_save_dir, self.graph)\n",
    "        \n",
    "            self.init_op = tf.global_variables_initializer()\n",
    "            # 注意如果不指定graph会使用默认graph，就获取不到在自定义的graph上的变量，报错 no variable to save \n",
    "            self.mySaver = tf.train.Saver(max_to_keep=2)\n",
    "\n",
    "    # ******** 初始化权重 ***********\n",
    "    def _initialize_weights(self):\n",
    "            multi_hot_field_size = self.multi_hot_field_size\n",
    "            one_hot_field_size = self.one_hot_field_size\n",
    "            numeric_field_size = self.numeric_field_size\n",
    "            feature_size = self.feature_size\n",
    "            embedding_size = self.embedding_size\n",
    "            deep_layers = self.deep_layers\n",
    "\n",
    "            weights = dict()\n",
    "            # embeddings\n",
    "            weights[\"feature_embeddings\"] = tf.Variable(\n",
    "                tf.random_normal([feature_size, embedding_size], -0.01, 0.01),\n",
    "                name=\"feature_embeddings\")  # feature_size * K\n",
    "            # FM first-order weights\n",
    "            weights[\"feature_bias\"] = tf.Variable(\n",
    "                tf.random_uniform([feature_size, 1], -0.01, 0.01), name=\"feature_bias\")  # feature_size * 1\n",
    "            # deep layers\n",
    "            # 总输入元个数为 : (涉及emb的特征个数) * embedding_size + 连续特征个数\n",
    "            input_size_emb = (multi_hot_field_size+one_hot_field_size) * embedding_size + numeric_field_size\n",
    "            glorot = np.sqrt(2.0 / (input_size_emb + deep_layers[0]))\n",
    "            weights[\"layer_0\"] = tf.Variable(\n",
    "                initial_value=np.random.normal(loc=0, scale=glorot, size=(input_size_emb, deep_layers[0])),\n",
    "                dtype=np.float32,\n",
    "                name=\"w_layer_0\")\n",
    "            weights[\"bias_0\"] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, deep_layers[0])),\n",
    "                                            dtype=np.float32, name=\"b_layer_0\")  # 1 * layers[0]\n",
    "            for i in range(1, len(deep_layers)):\n",
    "                glorot = np.sqrt(2.0 / (deep_layers[i - 1] + deep_layers[i]))\n",
    "                weights[\"layer_%d\" % i] = tf.Variable(\n",
    "                    np.random.normal(loc=0, scale=glorot, size=(deep_layers[i - 1], deep_layers[i])),\n",
    "                    dtype=np.float32, name=\"w_layer_%d\" % i)  # layers[i-1] * layers[i]\n",
    "                weights[\"bias_%d\" % i] = tf.Variable(\n",
    "                    np.random.normal(loc=0, scale=glorot, size=(1, deep_layers[i])),\n",
    "                    dtype=np.float32, name=\"b_layer_%d\" % i)  # 1 * layer[i]\n",
    "            # final concat projection layer\n",
    "            ################\n",
    "            # fm的y_first_order已经被提前求和了，所以只需要给它一个权重\n",
    "            # （因为在weights[\"feature_bias\"]中已经有部分作为“权重”乘上了y_first_order的特征值，然后求和，相当于每个一阶特征都有自己的隐向量x权重(来自w[\"feature_bias\"])\n",
    "            ################\n",
    "            cocnat_input_size_emb = 1 + embedding_size + deep_layers[-1]\n",
    "            glorot = np.sqrt(2.0 / (cocnat_input_size_emb + 1))\n",
    "            weights[\"concat_projection\"] = tf.Variable(\n",
    "                np.random.normal(loc=0, scale=glorot, size=(cocnat_input_size_emb, 1)),\n",
    "                dtype=np.float32, name=\"concat_projection\")  # layers[i-1]*layers[i]\n",
    "            weights[\"concat_bias\"] = tf.Variable(tf.constant(0.01), dtype=np.float32, name=\"concat_bias\")\n",
    "            return weights\n",
    "\n",
    "\n",
    "    # ******** deepfm ***********\n",
    "    def _deep_fm_graph(self,weights, feat_total_idx_sp, feat_total_value_sp,\n",
    "                          feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list,\n",
    "                          feat_numeric_sp, feat_category_sp, train_phase):\n",
    "            def batch_norm_layer(x, inp_train_phase, scope_bn,inp_batch_norm_decay):\n",
    "                bn_train = batch_norm(x, decay=inp_batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                                      is_training=True, reuse=None, trainable=True, scope=scope_bn)\n",
    "                bn_inference = batch_norm(x, decay=inp_batch_norm_decay, center=True, scale=True, updates_collections=None,\n",
    "                                          is_training=False, reuse=True, trainable=True, scope=scope_bn)\n",
    "                z = tf.cond(inp_train_phase, lambda: bn_train, lambda: bn_inference)\n",
    "                return z\n",
    "\n",
    "            dropout_keep_fm = self.dropout_fm\n",
    "            dropout_keep_deep = self.dropout_deep\n",
    "            numeric_feature_size = self.numeric_field_size\n",
    "            onehot_field_size = self.one_hot_field_size\n",
    "            multi_hot_field_size = self.multi_hot_field_size\n",
    "            embedding_size = self.embedding_size\n",
    "            deep_layers_activation = self.deep_layers_activation\n",
    "            batch_norm_decay = self.batch_norm_decay\n",
    "            deep_input_size = multi_hot_field_size + onehot_field_size\n",
    "            # ---------- FM component ---------\n",
    "            with tf.name_scope(\"FM\"):\n",
    "                # ---------- first order term ----------\n",
    "                with tf.name_scope(\"1st_order\"):\n",
    "                    y_first_order = tf.nn.embedding_lookup_sparse(\n",
    "                        weights[\"feature_bias\"],\n",
    "                        sp_ids=feat_total_idx_sp,\n",
    "                        sp_weights=feat_total_value_sp,\n",
    "                        combiner=\"sum\")\n",
    "                    y_first_order = tf.nn.dropout(\n",
    "                        y_first_order,\n",
    "                        dropout_keep_fm[0],\n",
    "                        name=\"y_first_order_dropout\")\n",
    "                # ---------- second order term ---------------\n",
    "                with tf.name_scope(\"2nd_order\"):\n",
    "                    # sum_square part\n",
    "                    summed_features_emb_square = tf.square(\n",
    "                        tf.nn.embedding_lookup_sparse(\n",
    "                            weights[\"feature_embeddings\"],\n",
    "                            sp_ids=feat_total_idx_sp,\n",
    "                            sp_weights=feat_total_value_sp,\n",
    "                            combiner=\"sum\"))\n",
    "                    # square_sum part\n",
    "                    squared_sum_features_emb = tf.nn.embedding_lookup_sparse(\n",
    "                        tf.square(weights[\"feature_embeddings\"]),\n",
    "                        sp_ids=feat_total_idx_sp,\n",
    "                        sp_weights=tf.square(feat_total_value_sp),\n",
    "                        combiner=\"sum\")\n",
    "                    # second order\n",
    "                    y_second_order = 0.5 * tf.subtract(\n",
    "                        summed_features_emb_square,\n",
    "                        squared_sum_features_emb)  # None * K\n",
    "                    y_second_order = tf.nn.dropout(y_second_order,\n",
    "                                                   dropout_keep_fm[1])  # None * K\n",
    "            # ---------- Deep component -------\n",
    "            with tf.name_scope(\"Deep\"):\n",
    "                # total_embedding 均值 用户的multi-hot one-hot特征都取到embedding作为DNN输入\n",
    "                with tf.name_scope(\"total_emb\"):\n",
    "                    # feat_one_hot = tf.sparse_add(feat_numeric_sp, feat_category_sp)\n",
    "                    feat_one_hot = feat_category_sp\n",
    "                    one_hot_embeddings = tf.nn.embedding_lookup(\n",
    "                        weights[\"feature_embeddings\"], feat_one_hot.indices[:, 1])\n",
    "                    one_hot_embeddings = tf.reshape(\n",
    "                        one_hot_embeddings,\n",
    "                        shape=(-1, onehot_field_size, embedding_size))\n",
    "                    multi_hot_embeddings = []\n",
    "                    for feat_idx_sp, feat_value_sp in zip(\n",
    "                            feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list):\n",
    "                        emb = tf.nn.embedding_lookup_sparse(\n",
    "                            weights[\"feature_embeddings\"],\n",
    "                            sp_ids=feat_idx_sp,\n",
    "                            sp_weights=feat_value_sp,\n",
    "                            combiner=\"mean\")\n",
    "                        emb = tf.reshape(emb, shape=[-1, 1, embedding_size])\n",
    "                        multi_hot_embeddings.append(emb)\n",
    "                    total_embeddings = tf.concat(\n",
    "                        values=[one_hot_embeddings] + multi_hot_embeddings, axis=1)\n",
    "                # input\n",
    "                with tf.name_scope(\"input\"):\n",
    "                    # 把连续特征不经过embedding直接输入到NN\n",
    "                    feat_numeric_sp_dense = tf.cast(\n",
    "                        tf.reshape(\n",
    "                            feat_numeric_sp.values, shape=(-1, numeric_feature_size)),\n",
    "                        tf.float32)\n",
    "                    y_deep_input = tf.reshape(\n",
    "                        total_embeddings,\n",
    "                        shape=[-1, deep_input_size * embedding_size])  # None * (F*K)\n",
    "                    y_deep_input = tf.concat([y_deep_input, feat_numeric_sp_dense],\n",
    "                                             axis=1)\n",
    "                    y_deep_input = tf.nn.dropout(y_deep_input, dropout_keep_deep[0])\n",
    "                # layer0\n",
    "                with tf.name_scope(\"layer0\"):\n",
    "                    y_deep_layer_0 = tf.add(\n",
    "                        tf.matmul(y_deep_input, weights[\"layer_0\"]), weights[\"bias_0\"])\n",
    "                    y_deep_layer_0 = batch_norm_layer(\n",
    "                        y_deep_layer_0, inp_train_phase=train_phase, scope_bn=\"bn_0\",inp_batch_norm_decay=batch_norm_decay)\n",
    "                    y_deep_layer_0 = deep_layers_activation(y_deep_layer_0)\n",
    "                    y_deep_layer_0 = tf.nn.dropout(y_deep_layer_0, dropout_keep_deep[1])\n",
    "                # layer1\n",
    "                with tf.name_scope(\"layer1\"):\n",
    "                    y_deep_layer_1 = tf.add(\n",
    "                        tf.matmul(y_deep_layer_0, weights[\"layer_1\"]),\n",
    "                        weights[\"bias_1\"])\n",
    "                    y_deep_layer_1 = batch_norm_layer(\n",
    "                        y_deep_layer_1, inp_train_phase=train_phase, scope_bn=\"bn_1\",inp_batch_norm_decay=batch_norm_decay)\n",
    "                    y_deep_layer_1 = deep_layers_activation(y_deep_layer_1)\n",
    "                    y_deep_layer_1 = tf.nn.dropout(y_deep_layer_1, dropout_keep_deep[2])\n",
    "                # layer2\n",
    "                with tf.name_scope(\"layer2\"):\n",
    "                    y_deep_layer_2 = tf.add(\n",
    "                        tf.matmul(y_deep_layer_1, weights[\"layer_2\"]),\n",
    "                        weights[\"bias_2\"])\n",
    "                    y_deep_layer_2 = batch_norm_layer(\n",
    "                        y_deep_layer_2, inp_train_phase=train_phase, scope_bn=\"bn_2\",inp_batch_norm_decay=batch_norm_decay)\n",
    "                    y_deep_layer_2 = deep_layers_activation(y_deep_layer_2)\n",
    "                    y_deep_layer_2 = tf.nn.dropout(y_deep_layer_2, dropout_keep_deep[3])\n",
    "                # layer3\n",
    "                with tf.name_scope(\"layer3\"):\n",
    "                    y_deep_layer_3 = tf.add(\n",
    "                        tf.matmul(y_deep_layer_2, weights[\"layer_3\"]),\n",
    "                        weights[\"bias_3\"])\n",
    "                    y_deep_layer_3 = batch_norm_layer(\n",
    "                        y_deep_layer_3, inp_train_phase=train_phase, scope_bn=\"bn_3\",inp_batch_norm_decay=batch_norm_decay)\n",
    "                    y_deep_layer_3 = deep_layers_activation(y_deep_layer_3)\n",
    "                    y_deep_layer_3 = tf.nn.dropout(y_deep_layer_3, dropout_keep_deep[4])\n",
    "            # ---------- DeepFM ---------------\n",
    "            with tf.name_scope(\"DeepFM\"):\n",
    "                concat_input = tf.concat(\n",
    "                    [y_first_order, y_second_order, y_deep_layer_3], axis=1)\n",
    "                out = tf.add(\n",
    "                    tf.matmul(concat_input, weights[\"concat_projection\"]),\n",
    "                    weights[\"concat_bias\"])\n",
    "\n",
    "            return tf.nn.sigmoid(out)\n",
    "\n",
    "\n",
    "    # ******** 构造input并触发deepfm计算 ***********\n",
    "    def run_deepfm(self,weights,inp_list,train_phase):\n",
    "        def __add_idx_to_tensor(inp_tensor):\n",
    "            idx = tf.range(tf.shape(inp_tensor)[0])\n",
    "            idx_2d = tf.reshape(idx,[-1,1])\n",
    "            idx_2d_full = tf.cast(tf.tile(idx_2d,[1,tf.shape(inp_tensor)[1]]),dtype=inp_tensor.dtype)\n",
    "            added = tf.concat([tf.reshape(idx_2d_full,[-1,1]),tf.reshape(inp_tensor,[-1,1])],axis=1)\n",
    "            return added\n",
    "\n",
    "        def _get_numeric_sp(inp_dict):\n",
    "            if len(self.global_numeric_fields) !=0:\n",
    "                idx_to_stack=[]\n",
    "                value_to_stack=[]\n",
    "                for field in self.global_numeric_fields:\n",
    "                    idx_to_stack.append(inp_dict[field+\"_idx\"])\n",
    "                    value_to_stack.append(inp_dict[field])\n",
    "                idx_dense = __add_idx_to_tensor(tf.transpose(tf.stack(idx_to_stack)))\n",
    "                value_dense = tf.reshape(tf.transpose(tf.stack(value_to_stack)),[-1])\n",
    "            else:\n",
    "                # 为了保持连贯性，没有连续特征会构造“一个”连续特征，全为0\n",
    "                idx_dense = tf.constant([[i,0] for i in range(self.batch_size)],dtype=tf.int64)\n",
    "                value_dense = tf.constant([0.0]*self.batch_size,dtype=tf.float32)\n",
    "            return tf.SparseTensor(indices=idx_dense, values=value_dense, dense_shape=[self.batch_size,self.numeric_field_size])\n",
    "\n",
    "        def _get_category_sp(inp_dict):\n",
    "            idx_dense = tf.constant([[0,0]],dtype=tf.int64)\n",
    "            value_dense = tf.constant([0.0],dtype=tf.float32)\n",
    "            if len(self.global_one_hot_fields) != 0:\n",
    "                idx_to_stack=[]\n",
    "                value_to_stack=[]\n",
    "                for field in self.global_one_hot_fields:\n",
    "                    idx_to_stack.append(inp_dict[field])\n",
    "                    value_to_stack.append(tf.ones_like(inp_dict[field],dtype=tf.float32))\n",
    "                    idx_dense = __add_idx_to_tensor(tf.transpose(tf.stack(idx_to_stack)))\n",
    "                    value_dense = tf.reshape(tf.transpose(tf.stack(value_to_stack)),[-1])\n",
    "            return tf.SparseTensor(indices=idx_dense, values=value_dense, dense_shape=self.global_dense_shape)\n",
    "\n",
    "        def _get_multi_hot_idx_list(inp_dict):\n",
    "            multi_hot_idx_list = []\n",
    "            if len(self.global_multi_hot_fields) != 0:\n",
    "                for field in self.global_multi_hot_fields:\n",
    "                    multi_hot_idx_list.append(inp_dict[field])\n",
    "            else:\n",
    "                multi_hot_idx_list.append(tf.SparseTensor(indices=[[0,0]], values=[0.0], dense_shape=self.global_dense_shape))\n",
    "            return multi_hot_idx_list\n",
    "\n",
    "        def _make_multi_hot_value_list(feat_idx_list):\n",
    "            multi_hot_value_list = []\n",
    "            if len(feat_idx_list) !=0:\n",
    "                multi_hot_value_list=[tf.SparseTensor(indices=sparse.indices,values=tf.ones_like(sparse.values,dtype=tf.float32),dense_shape=sparse.dense_shape) for sparse in feat_idx_list]\n",
    "            else:\n",
    "                multi_hot_value_list.append(tf.SparseTensor(indices=[[0,0]], values=[0.0], dense_shape=self.global_dense_shape))\n",
    "            return multi_hot_value_list\n",
    "\n",
    "        def _get_total_feature(inp_dict):\n",
    "            idx_to_stack = []\n",
    "            value_to_stack = []\n",
    "            # sparse_tensor来表示multi_hot\n",
    "            multi_hot_idx_sparse_list = []\n",
    "            for field in self.global_all_fields:\n",
    "                if field in self.global_multi_hot_fields:\n",
    "                    multi_hot_idx_sparse_list.append(inp_dict[field])\n",
    "                if field in self.global_numeric_fields:\n",
    "                    idx_to_stack.append(inp_dict[field+\"_idx\"])\n",
    "                    value_to_stack.append(inp_dict[field])\n",
    "                    pass\n",
    "                if field in self.global_one_hot_fields:\n",
    "                    idx_to_stack.append(inp_dict[field])\n",
    "                    value_to_stack.append(tf.ones_like(inp_dict[field],dtype=tf.float32))\n",
    "                    pass\n",
    "            # sparse_tensor的values中原来都是特征索引，替换成1.0\n",
    "            multi_hot_value_sparse_list = [tf.SparseTensor(indices=sparse.indices, values=tf.ones_like(sparse.values,dtype=tf.float32), dense_shape=sparse.dense_shape) for sparse in multi_hot_idx_sparse_list]\n",
    "            # idx sparse of numeric+onehot\n",
    "            idx_dense = tf.transpose(tf.stack(idx_to_stack))\n",
    "            idx_sparse = tf.contrib.layers.dense_to_sparse(tensor=idx_dense,eos_token=-1)\n",
    "            # value sparse of numeric+onehot\n",
    "            value_dense = tf.transpose(tf.stack(value_to_stack))\n",
    "            value_sparse = tf.contrib.layers.dense_to_sparse(tensor=value_dense,eos_token=-1)\n",
    "\n",
    "            total_idx_sparse = tf.sparse_concat(axis=1,sp_inputs=[idx_sparse]+ multi_hot_idx_sparse_list)\n",
    "            total_value_sparse = tf.sparse_concat(axis=1,sp_inputs=[value_sparse] + multi_hot_value_sparse_list)\n",
    "            return total_idx_sparse, total_value_sparse\n",
    "        with tf.name_scope(\"gen_feat_total\"):\n",
    "            feat_total_idx_sp,feat_total_value_sp = _get_total_feature(inp_list)\n",
    "            self.feat_total_idx_sp,self.feat_total_value_sp = feat_total_idx_sp,feat_total_value_sp\n",
    "        with tf.name_scope(\"gen_feat_multi_hot\"):\n",
    "            feat_multi_hot_idx_sp_list = _get_multi_hot_idx_list(inp_list)\n",
    "            self.feat_multi_hot_idx_sp_list = feat_multi_hot_idx_sp_list\n",
    "            feat_multi_hot_value_sp_list = _make_multi_hot_value_list(feat_multi_hot_idx_sp_list)\n",
    "            self.feat_multi_hot_value_sp_list = feat_multi_hot_value_sp_list\n",
    "        with tf.name_scope(\"gen_feat_numeric\"):\n",
    "            feat_numeric_sp = _get_numeric_sp(inp_list)\n",
    "            self.feat_numeric_sp = feat_numeric_sp\n",
    "        with tf.name_scope(\"gen_feat_category\"):\n",
    "            feat_category_sp = _get_category_sp(inp_list)\n",
    "            self.feat_category_sp = feat_category_sp\n",
    "\n",
    "        return self._deep_fm_graph(weights,feat_total_idx_sp, feat_total_value_sp,\n",
    "                          feat_multi_hot_idx_sp_list, feat_multi_hot_value_sp_list,\n",
    "                          feat_numeric_sp, feat_category_sp, train_phase)\n",
    "\n",
    "    # ******** 初始化计算图 ***********\n",
    "    def _init_graph(self):\n",
    "        with self.graph.as_default():\n",
    "            weights = self._initialize_weights()\n",
    "            total_parameters = 0\n",
    "            for variable in weights.values():\n",
    "                shape = variable.get_shape()\n",
    "                variable_parameters = 1\n",
    "                for dim in shape:\n",
    "                    variable_parameters *= dim.value\n",
    "                total_parameters += variable_parameters\n",
    "            print(\"total_parameters cnt : %s\" % total_parameters)\n",
    "\n",
    "            inp_tfrecord_path = tf.placeholder(dtype=tf.string, name=\"tfrecord_path\")\n",
    "            inp_iterator = get_iterator(inp_tfrecord_path,self.global_all_fields,self.global_multi_hot_fields,self.global_numeric_fields,self.max_numeric,self.tmp_map_num_f,self.batch_size)\n",
    "            inp_next_dict = inp_iterator.get_next()\n",
    "            self.inp_next_dict = inp_next_dict\n",
    "            # prepare\n",
    "            # inp_next_dict     key: decode时使用的字符串，value: tensor\n",
    "            # placeholder_dict  key: decode时使用的字符串，value: placeholder\n",
    "            #                   目的：为了让后面的流程都使用placeholder进行,这样存储模型可以以这些placeholder为输入口\n",
    "            # ori_feed_dict     key: placeholder        value: tensor\n",
    "            #                   目的：直接sess.run(ori_feed_dict)就可以得到后续流程需要的placeholder的feed_dict;\n",
    "            # 构造placeholder输入，方便模型文件restore后的使用\n",
    "            # 这里实际上只是把 inp_next 这个“源字典”的 value 都用placeholder替换了，key未变\n",
    "            placeholder_dict = {}\n",
    "            with tf.name_scope(\"input\"):\n",
    "                # train_phase放到这里只是为了共享同一个name_scope\n",
    "                train_phase = tf.placeholder(dtype=tf.bool,name=\"train_phase\")\n",
    "                placeholder_dict[\"train_phase\"]=train_phase\n",
    "                for k,v in inp_next_dict.items():\n",
    "                    if k in self.global_multi_hot_fields:\n",
    "                        placeholder_dict[k]=tf.sparse_placeholder(dtype=tf.int64,name=k)\n",
    "                    elif k in self.global_numeric_fields:\n",
    "                        placeholder_dict[k]=tf.placeholder(dtype=tf.float32,name=k)\n",
    "                    else:\n",
    "                        placeholder_dict[k]=tf.placeholder(dtype=tf.int64,name=k)\n",
    "                # 构造一个feed_dict在训练的时候自动就用它，取placeholder为key，取“源字典”的value为value\n",
    "                ori_feed_dict = {placeholder_dict[k] : inp_next_dict[k] for k,v in inp_next_dict.items()}\n",
    "            self.placeholder_dict = placeholder_dict\n",
    "            \n",
    "            # deepfm\n",
    "            deepfm_output = self.run_deepfm(weights,placeholder_dict,train_phase)\n",
    "            with tf.name_scope(\"output\"):\n",
    "                pred = tf.reshape(deepfm_output,[-1],name=\"pred\")\n",
    "            # label\n",
    "            label_op = placeholder_dict['label']\n",
    "\n",
    "            # loss\n",
    "            empirical_risk = tf.reduce_mean(tf.losses.log_loss(label_op, pred))\n",
    "            loss_op = empirical_risk\n",
    "            if self.l2_reg>0:\n",
    "                structural_risk = tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"concat_projection\"])\n",
    "                for i in range(len(self.deep_layers)):\n",
    "                    structural_risk += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"layer_%d\"%i])\n",
    "                tf.summary.scalar('structural_risk_L2',structural_risk)\n",
    "                loss_op = empirical_risk + structural_risk\n",
    "\n",
    "            # loss_op = tf.reduce_mean(tf.losses.log_loss(label_op, pred))\n",
    "            if self.l2_reg>0:\n",
    "                loss_op += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"concat_projection\"])\n",
    "                for i in range(len(self.deep_layers)):\n",
    "                    loss_op += tf.contrib.layers.l2_regularizer(self.l2_reg)(weights[\"layer_%d\"%i])\n",
    "\n",
    "            # optimizer\n",
    "            _optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate, beta1=0.9, beta2=0.999,epsilon=1e-8)\n",
    "            grad = _optimizer.compute_gradients(loss_op)\n",
    "            optimize_op = _optimizer.apply_gradients(grad)\n",
    "\n",
    "            # summary (tensorboard)\n",
    "            tf.summary.scalar('total_loss', loss_op)\n",
    "            tf.summary.scalar('empirical_risk_logloss',empirical_risk)\n",
    "            for g,v in grad:\n",
    "                if g is not None:\n",
    "                    _=tf.summary.histogram(v.op.name+\"/gradients\",g)\n",
    "            for v in tf.trainable_variables():\n",
    "                _=tf.summary.histogram(v.name.replace(\":0\",\"/value\"),v)\n",
    "            \n",
    "            inputs_dict = placeholder_dict\n",
    "            outputs_dict = {\"pred\":pred}\n",
    "        return inp_tfrecord_path,inp_iterator,optimize_op,inputs_dict,outputs_dict,weights,ori_feed_dict,loss_op\n",
    "\n",
    "    def _evaluate(self,sess,valid_dict):\n",
    "        pred_deque,label_deque=deque(),deque()\n",
    "        batch_cnt = 0\n",
    "        while True:\n",
    "            try:\n",
    "                valid_dict.update(sess.run(self.ori_feed_dict))\n",
    "                batch_cnt += 1\n",
    "                t1 = time.time()\n",
    "                pred_,label_ = sess.run([self.pred,self.label_op],valid_dict)\n",
    "                pred_deque.extend(pred_)\n",
    "                label_deque.extend(label_)\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                sys.stdout.write(\"\\n\")\n",
    "                sys.stdout.flush()\n",
    "                break\n",
    "            delta_t = time.time() - t1\n",
    "            sys.stdout.write(\"    valid_batch_cnt: [{batch_cnt:0>3d}] [{delta_t:.2f}s/per]\\r\".format(batch_cnt=batch_cnt,delta_t=delta_t))\n",
    "            sys.stdout.flush()\n",
    "        pred_arr = np.array(pred_deque)\n",
    "        label_arr = np.array(label_deque)\n",
    "        auc = roc_auc_score(label_arr,pred_arr)\n",
    "        loss = log_loss(label_arr,pred_arr,eps=1e-7)\n",
    "        return loss,auc\n",
    "\n",
    "    def _simple_save(self,sess,path,inputs,outputs,global_batch_cnt,auc,use_simple_save = False):\n",
    "        print(\"save model at %s\" % path)\n",
    "        if use_simple_save:\n",
    "            tf.saved_model.simple_save(sess,path+\"/model_of_auc-{auc:.5f}\".format(auc=auc),inputs,outputs)\n",
    "        else:\n",
    "            self.mySaver.save(sess, path+\"/model.ckpt\", global_step=global_batch_cnt)\n",
    "        pass\n",
    "    \n",
    "    def fit(self):\n",
    "        train_feed={self.train_phase:True, self.inp_tfrecord_path:self.train_tfrecord_file}\n",
    "        valid_feed={self.train_phase:False, self.inp_tfrecord_path:self.valid_tfrecord_file}\n",
    "        # 不适用self.sess,因为必须在with结构内触发保存模型才能存住variable\n",
    "        # 如果使用 with self.sess as sess，则with结束后self.sess直接销毁了，没有意义\n",
    "        model_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "        model_config.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(graph=self.graph,config=model_config)\n",
    "        with sess as sess:\n",
    "            sess.run(self.init_op)\n",
    "            global_auc,global_batch_cnt,batch_cnt,epoch_cnt =0,0,0,0\n",
    "            for epoch in range(self.epoch):\n",
    "                epoch_cnt += 1\n",
    "                batch_cnt = 0\n",
    "                sess.run(self.inp_iterator.initializer,train_feed)\n",
    "                t0=time.time()\n",
    "                while True:\n",
    "                    try:\n",
    "                        batch_cnt += 1\n",
    "                        global_batch_cnt += 1\n",
    "                        # 加上代表placeholder和其value的键值 | train_feed目前只有train_phase和tfrecord_path这些外部参数\n",
    "                        train_feed.update(sess.run(self.ori_feed_dict))\n",
    "                        run_ops=[self.optimize_op,self.loss_op,self.pred,self.label_op,self.merge_summary]\n",
    "                        run_result = sess.run(run_ops,train_feed)\n",
    "                        _,loss_,pred_,label_,merge_summary_ = run_result\n",
    "                        self.writer.add_summary(merge_summary_,batch_cnt)\n",
    "                        if batch_cnt % 100 == 0:\n",
    "                            auc = roc_auc_score(label_,pred_)\n",
    "                            batch_time = time.time()-t0\n",
    "                            myprint(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d}] logloss:[{loss_:.5f}] auc:[{auc:.5f}] [{batch_time:.1f}s]\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt,loss_=loss_,auc=auc,batch_time=batch_time))\n",
    "                            t0=time.time()\n",
    "                        # 存在严重缺陷导致不能在中途显示验证集auc logloss，\n",
    "                        # 这里如果用valid初始化iterator后，从1001batch开始都会从valid里面拿数据了\n",
    "            #             if batch_cnt % 1000 ==0:\n",
    "            #                 sess.run(inp_iterator.initializer,valid_dict)\n",
    "            #                 logloss,auc=_evaluate(sess,valid_feed)\n",
    "            #                 now = time.strftime(\"|%Y-%m-%d %H:%M:%S| \", time.localtime(time.time()))\n",
    "            #                 print(f\"{now} [e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d} valid] logloss:[{logloss:.5f}] auc:[{auc:.5f}]\")\n",
    "                    except tf.errors.OutOfRangeError:\n",
    "                        break\n",
    "                myprint(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d}] epoch-done\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt))\n",
    "                sess.run(self.inp_iterator.initializer,valid_feed)\n",
    "                logloss,auc=self._evaluate(sess,valid_feed)\n",
    "                myprint(\"[e:{epoch_cnt:0>2d}|b:{batch_cnt:0>4d} valid] valid_logloss:[{logloss:.5f}] valid_auc:[{auc:.5f}]\".format(epoch_cnt=epoch_cnt,batch_cnt=batch_cnt,logloss=logloss,auc=auc))\n",
    "                if global_auc<auc:\n",
    "                    global_auc = auc\n",
    "                    myprint(\"logloss:[{logloss:.5f}] auc:[{auc:.5f}] global_batch_cnt:[{global_batch_cnt:0>4d}] gonna save model ...\".format(logloss=logloss,auc=auc,global_batch_cnt=global_batch_cnt))\n",
    "                    self._simple_save(sess,self.model_save_dir,self.inputs_dict,self.outputs_dict,global_batch_cnt,auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T03:54:03.331321Z",
     "start_time": "2018-12-04T03:53:49.822801Z"
    }
   },
   "outputs": [],
   "source": [
    "CONFIG = config_midas()\n",
    "process = DeepFM(CONFIG.train_tfrecord_file,CONFIG.valid_tfrecord_file,CONFIG.random_seed,CONFIG.base_save_dir,CONFIG.deepfm_param_dicts,CONFIG.data_param_dicts)\n",
    "self = process\n",
    "train_feed={self.train_phase:True, self.inp_tfrecord_path:self.train_tfrecord_file}\n",
    "valid_feed={self.train_phase:False, self.inp_tfrecord_path:self.valid_tfrecord_file}\n",
    "model_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "model_config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(graph=self.graph,config=model_config)\n",
    "sess.run(self.init_op)\n",
    "sess.run(self.inp_iterator.initializer,train_feed)\n",
    "train_feed.update(sess.run(self.ori_feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T03:54:03.334502Z",
     "start_time": "2018-12-04T03:54:03.332677Z"
    }
   },
   "outputs": [],
   "source": [
    "inp_dict = self.placeholder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T03:54:03.377649Z",
     "start_time": "2018-12-04T03:54:03.336181Z"
    }
   },
   "outputs": [],
   "source": [
    "def __add_idx_to_tensor(inp_tensor):\n",
    "            idx = tf.range(tf.shape(inp_tensor)[0])\n",
    "            idx_2d = tf.reshape(idx,[-1,1])\n",
    "            idx_2d_full = tf.cast(tf.tile(idx_2d,[1,tf.shape(inp_tensor)[1]]),dtype=inp_tensor.dtype)\n",
    "            added = tf.concat([tf.reshape(idx_2d_full,[-1,1]),tf.reshape(inp_tensor,[-1,1])],axis=1)\n",
    "            return added\n",
    "idx_to_stack=[]\n",
    "value_to_stack=[]\n",
    "for field in self.global_numeric_fields:\n",
    "    idx_to_stack.append(inp_dict[field+\"_idx\"])\n",
    "    value_to_stack.append(inp_dict[field])\n",
    "idx_dense = __add_idx_to_tensor(tf.transpose(tf.stack(idx_to_stack)))\n",
    "value_dense = tf.reshape(tf.transpose(tf.stack(value_to_stack)),[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T04:02:16.453168Z",
     "start_time": "2018-12-04T04:02:16.429712Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train_feed[self.placeholder_dict['stat_ad_creative_id_s__cvr_3d']]\n",
    "# sess.run(self.ori_feed_dict[self.placeholder_dict['stat_ad_creative_id_s__cvr_3d']])\n",
    "# sess.run(self.inp_next_dict['stat_ad_creative_id_s__cvr_3d'])\n",
    "# sess.run(self.placeholder_dict['stat_ad_creative_id_s__cvr_3d'],train_feed)\n",
    "self.inp_next_dict['stat_ad_creative_id_s__cvr_3d']\n",
    "# sess.run([idx_dense,value_dense],train_feed)\n",
    "for k,v in self.ori_feed_dict.items():\n",
    "    k,v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T03:21:10.264764Z",
     "start_time": "2018-12-04T03:21:10.111751Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "a=sess.run([self.feat_numeric_sp,self.feat_category_sp],train_feed)\n",
    "a[0]\n",
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T04:42:10.722648Z",
     "start_time": "2018-11-09T04:42:10.716701Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "(1,2,3,\"34\",[23,2],None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0a4b0220565be4badc3b4fdfcf5de99d"
  },
  "celltoolbar": "Initialization Cell",
  "gist": {
   "data": {
    "description": "py_script/DeepFM_PackageMode.ipynb",
    "public": false
   },
   "id": "0a4b0220565be4badc3b4fdfcf5de99d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
